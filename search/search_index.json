{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the documentation for the Oil Price Prediction Project. This project revolves around predicting oil prices using various machine learning models and presenting the results through a user-friendly web interface which can be accessed here.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project is for the Software Engineering course at Columbus State University.</p> <p>Throughout this project, we've been working with a client from the Economics department at our school. Our client has expressed an interest in contrasting the predictive capability of machine learning models against traditional econometric forecasting methods. Through this project, we aim to explore the synergies between technology and economics to achieve more accurate and insightful predictions.</p> <p>It is a collaborative effort by The Oval Table: a team of four students pursuing their Bachelor's in Computer Science.  The team members are:</p> <ul> <li>Maria Mills</li> <li>Jae Kim</li> <li>William Hutto</li> <li>Gurpreet Singh</li> </ul>"},{"location":"#client-requirements","title":"Client Requirements","text":"<p>Our project's primary directive stems from the detailed requirements provided by our client, a professor within the economics department. These requirements have shaped the direction and emphasis of our analysis and modeling. Here's a comprehensive breakdown of our client's specifications:</p>"},{"location":"#data-primary-processing","title":"Data Primary Processing","text":"<ol> <li> <p>Real vs Nominal Oil Price:</p> <p>Objective: Convert nominal oil prices to real oil prices to accurately capture market dynamics by adjusting for inflation.</p> <p>Formula:</p> <p><code>Real Price = Nominal Price/CPI \u00d7 100</code></p> <ul> <li> <p>Nominal Oil Price: As provided in the dataset (before inflation adjustment).</p> </li> <li> <p>CPI (Consumer Price Index): This reflects the inflation rate and average price changes over time. For our dataset, CPI (All items) can be found as entry #105 in the macroeconomic data.</p> </li> </ul> <p>Distinction:</p> <p>Nominal Price: Not adjusted for inflation.</p> <p>Real Price: Adjusted for inflation, offering a clearer representation of market dynamics devoid of inflation-induced noise.</p> </li> <li> <p>Log Transformation:</p> <p>Objective: Stabilize variance across macroeconomic variables, which exhibit significant relative scale variation.</p> <p>Transformation:</p> <p><code>x -&gt; log(x) (base 2)</code></p> <p>Exceptions:</p> <ul> <li>Variables expressed as percentages, such as:<ul> <li>Unemployment Rate</li> <li>Interest Rates</li> </ul> </li> </ul> </li> </ol>"},{"location":"#analysis-instructions","title":"Analysis Instructions","text":"<p>Descriptive Statistics:</p> <ol> <li> <p>Generate descriptive statistics for the transformed real oil price to understand its distribution, central tendency, and variability.</p> </li> <li> <p>Prediction Goal:</p> <ul> <li>Predict the near-future real oil price using the Macroeconomic Data at time t.</li> <li>One month ahead (t+1)</li> <li>Potentially extend to 3, 6, or 12 months in the future based on model performance and feasibility.</li> </ul> </li> </ol>"},{"location":"#data-splitting-strategy","title":"Data Splitting Strategy:","text":"<ul> <li> <p>Initially for Cycle 1 we used an 80%/20% train-test split for modeling.</p> </li> <li> <p>We plan to also try 50/50 split for Cycle 2, as per client's request.</p> </li> </ul>"},{"location":"#project-goals","title":"Project Goals","text":"<ul> <li> <p>Data Cleaning: Clean and preprocess the data to make it suitable for machine learning.</p> </li> <li> <p>Feature Engineering: Identify relevant features that enhance the predictive power of our models.</p> </li> <li> <p>Modeling: Implement and evaluate various machine learning models, optimizing for accuracy and precision in predictions.</p> </li> <li> <p>Web Interface: Develop a user-friendly web application where users can view predictions, understand model metrics, and even run their own predictions with selected models and timeframes.</p> </li> <li> <p>Documentation: Maintain comprehensive documentation (which you're reading now) detailing every facet of the project, from data processing to user interface guidance.</p> </li> </ul>"},{"location":"#why-oil-price-prediction","title":"Why Oil Price Prediction?","text":"<p>We were only given a choice between three projects, and this one seemed the most interesting. But in all seriousness, oil is a vital commodity that affects the global economy in a myriad of ways. It's also a highly volatile commodity, with prices fluctuating wildly over the years. This makes it an ideal candidate for machine learning, as it's a complex problem that can benefit from the predictive power of modern algorithms.</p>"},{"location":"#navigating-this-documentation","title":"Navigating This Documentation","text":"<p>This documentation is structured to provide a comprehensive view of the project:</p> <ul> <li>About: An overview of the project, its goals, and the team behind it.</li> <li>Data: Dive deep into the data used for this project, including its sources, structure, and processing. </li> <li>Models: Learn about each model implemented, their results, and the metrics that showcase their performance.</li> <li>Web Interface: An overview of the web application, its features, and how to run it.</li> <li>Challenges: A detailed look at the challenges faced during the project, and the solutions that were implemented to overcome them.</li> <li>FAQ: A list of frequently asked questions about the project.</li> <li>Conclusion: A summary of the project, its results, and the lessons learned.</li> </ul> <p>We invite you to explore the documentation and gain insight into the meticulous processes, challenges, and innovative solutions that have shaped this project.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>We would like to express our deepest appreciation to everyone who made it possible for us to complete this project. Our heartfelt gratitude goes to our client, Dr. Wen Shi, whose invaluable suggestions and encouragement were instrumental in guiding our project, such as the preparation of this documentation.</p> <p>Furthermore, we extend our sincere thanks to the dedicated staff of the Computer Science Department at Columbus State University, specifically Dr. Mohamed Abid and Dr. Yi Zhou. Their guidance, innovative ideas, and access to their knowledge played a crucial role in bringing our project to life.</p> <p>A special acknowledgment goes to our team, The Oval Table, whose unwavering support and consistent guidance were vital in completing our work both effectively and efficiently. We are grateful to each member for their commitment, open communication, and collaborative spirit throughout this challenging academic journey.</p> <p>Thank you. \u2764\ufe0f</p>"},{"location":"challenges/","title":"Challenges","text":"<p>Throughout the course of this project, we encountered a myriad of challenges that tested our skills, patience, and understanding of machine learning and data science. </p> <p>Here\u2019s a comprehensive overview of the hurdles we faced and the lessons learned:</p> <p>These challenges, while daunting at times, were instrumental in our learning journey. They pushed us out of our comfort zones, forcing us to think critically, solve problems, and continually learn and adapt. The skills and knowledge gained through overcoming these challenges have been invaluable and will undoubtedly serve us well in future endeavors.</p>"},{"location":"challenges/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing","text":"<p>One of the initial and most time-consuming (and ongoing) challenges was cleaning and preprocessing the data. Real-world data is rarely perfect; it comes with inconsistencies, missing values, and outliers. We had to employ various techniques to handle these issues, ensuring the quality and reliability of our dataset. Learning to identify and manage these discrepancies was a crucial step in our journey.</p>"},{"location":"challenges/#overfitting-and-underfitting","title":"Overfitting and Underfitting","text":"<p>Striking the right balance between model complexity and training data performance was another hurdle. We grappled with overfitting, where our model performed exceptionally well on the training data but failed to generalize on unseen data. Conversely, underfitting was also a challenge, with models too simplistic to capture the underlying patterns of the data. Understanding and mitigating these issues were pivotal in enhancing our model's performance.</p>"},{"location":"challenges/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Determining the optimal set of hyperparameters for our models was akin to finding a needle in a haystack. We experimented with various combinations, learning the art of balancing bias and variance to improve our models. This process was both challenging and enlightening, as it underscored the impact of hyperparameters on model performance.</p>"},{"location":"challenges/#regularization","title":"Regularization","text":"<p>Implementing regularization techniques to prevent overfitting was a new concept for us. We explored L1 and L2 regularization methods, gaining insights into how these techniques add penalty terms to the cost function, encouraging simpler models and mitigating overfitting.</p>"},{"location":"challenges/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Choosing the right evaluation metrics was crucial in assessing our models accurately. We delved into various metrics like accuracy, precision, recall, and F1 score, learning to select the most appropriate metric based on the nature of our problem and the dataset.</p>"},{"location":"challenges/#computational-resources-and-efficiency","title":"Computational Resources and Efficiency","text":"<p>As we worked with large datasets and complex models, the limitations of our computational resources became apparent. We learned to optimize our code, use efficient libraries, and sometimes compromise on model complexity to ensure feasible computation times.</p>"},{"location":"challenges/#managing-expectations-and-patience","title":"Managing Expectations and Patience","text":"<p>Machine learning, we realized, is not always about achieving the perfect model. It\u2019s about experimentation, iteration, and continuous learning. Managing our expectations and maintaining patience during seemingly endless cycles of training and tuning was a challenge in itself.</p>"},{"location":"challenges/#documentation-and-code-organization","title":"Documentation and Code Organization","text":"<p>Maintaining clear and comprehensive documentation of our code, experiments, and findings was essential yet challenging. We learned the importance of code organization, commenting, and documentation for future reference and for the benefit of all team members.</p>"},{"location":"conclusions/","title":"Conclusions","text":"<p>As we approach the culmination of our project, we take a moment to reflect on the journey, the learning curves, and the invaluable experiences gained along the way. Embarking on this endeavor with minimal prior knowledge in Machine Learning (ML), we now find ourselves more adept and confident in navigating the complexities of ML, model training, and data preprocessing.</p> <p>The project served as a practical introduction to the world of Machine Learning, providing us with hands-on experience in various critical aspects of the field. We delved deep into data cleaning and preprocessing, understanding the pivotal role that quality data plays in building robust ML models. Through trial and error, we learned to handle missing values, outliers, and other data inconsistencies that are part and parcel of real-world datasets.</p> <p>Model training and hyperparameter tuning were other significant factors of this learning journey. We experimented with different ML models, tweaking and tuning hyperparameters to achieve better performance. This process not only sharpened our technical skills but also cultivated a keen sense of patience and attention to detail\u2014attributes essential in the field of ML.</p> <p>Our project also served as a practical introduction to a suite of Python libraries, each playing a vital role in the data science and ML landscape. </p> <ul> <li>Matplotlib enabled us to visualize data and results, bringing clarity to complex datasets. </li> <li>Scikit-learn provided a robust and user-friendly framework for implementing various ML algorithms.</li> <li>Pandas became our go-to library for data manipulation and analysis. </li> <li>NumPy facilitated the efficient handling of large datasets, enabling us to perform complex mathematical operations with ease.</li> <li>Jupyter Notebook and Conda facilitated a seamless development environment, allowing us to experiment, iterate, and document our work efficiently.</li> </ul> <p>While the project is still ongoing, the knowledge and experience gained thus far are invaluable. We have transitioned from ML novices to practitioners with a better understanding in the field, ready to tackle more complex challenges and continue our learning journey. The project has not just been a technical journey but also a testament to the power of perseverance, collaboration, and a relentless pursuit of knowledge.</p> <p>In conclusion, this project has been a transformative experience, exploring the world of Machine Learning and equipping us with the skills and knowledge to forge ahead in this exciting and ever-evolving field.</p>"},{"location":"design/","title":"Design Overview: Oil Price Prediction Project","text":"<p>Hello! In this document, we\u2019re going to talk about how we designed our Oil Price Prediction Project from start to finish. It\u2019s like when you sketch out your ideas before building something awesome \u2013 and that\u2019s exactly what we did!</p>"},{"location":"design/#initial-thoughts-and-sketches","title":"Initial Thoughts and Sketches","text":"<p>When we first started this project, we knew we wanted to create something that could predict future oil prices, and we also wanted to share our findings through a web interface. So, we started by sketching out our ideas on paper. This helped us visualize our thoughts and communicate them with each other.</p>"},{"location":"design/#sequence-diagrams-version-1-and-version-2","title":"Sequence Diagrams: Version 1 and Version 2","text":"<p>We created two sequence diagrams to help us understand how different parts of our project would interact with each other.</p> <ol> <li> <p>Sequence Diagram V1: This was our initial concept. It showed how the user would interact with our model, inputting data and receiving predictions.  </p> </li> <li> <p>Sequence Diagram V2: This was our initial concept with the idea of a frontend web interface. It showed how the user would interact with our model through the web interface, inputting data and receiving predictions. </p> </li> </ol>"},{"location":"design/#data-flow-diagram-model-training-process","title":"Data Flow Diagram: Model Training Process","text":"<p>Next, we needed to understand how data would move through our system, especially during the model training process. So, we drew a Data Flow Diagram (DFD) to visualize this. It showed how data would be inputted, processed, and outputted, helping us ensure that every piece of data was accounted for and properly handled. </p>"},{"location":"design/#program-structure-diagram-model-training-process","title":"Program Structure Diagram: Model Training Process","text":"<p>Finally, we wanted to map out the structure of our program, focusing on the model training process. We created a Program Structure Diagram to do this. It outlined the main components of our system, how they were connected, and how data would flow between them. This helped us ensure that our program was organized and efficient. </p>"},{"location":"design/#design-evolution-and-lessons-learned","title":"Design Evolution and Lessons Learned","text":"<p>As we worked on our project, we learned a lot and made adjustments to our design. We realized that clear and detailed planning at the start could save us a lot of time and confusion later on. We also learned the importance of being flexible and open to changes, especially when incorporating new features like the web interface.</p>"},{"location":"design/#final-thoughts","title":"Final Thoughts","text":"<p>In the end, our design process was a crucial part of our project. It helped us understand our own ideas, communicate with each other, and build a solution that we\u2019re really proud of. And we hope that by sharing this overview, you can get a glimpse into how we turned our sketches and diagrams into a real, working project!</p> <p>Happy exploring and designing!</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#what-is-the-main-objective-of-this-project","title":"What is the main objective of this project?","text":"<p>Our primary goal is to leverage machine learning to predict oil prices with high precision. This project stems from our Software Engineering course (CPSC 4175), where we were paired with \"clients\" from within the academic institution. Our assigned 'client' stems from the economics department and is interested in contrasting the predictive capability of machine learning models against traditional econometric forecasting methods. Through this project, we aim to explore the synergies between technology and economics to achieve more accurate and insightful predictions.</p>"},{"location":"faq/#why-did-you-start-with-regression-models-in-cycle-1","title":"Why did you start with Regression models in Cycle 1?","text":"<p>As newcomers to machine learning, we wanted to initiate our journey with a foundational understanding. Regression models, such as XGBoost, Random Forest, and Polynomial Regression, provided a suitable starting point. These models served as our initial attempt to understand machine learning concepts, oil price trends and correlations.</p>"},{"location":"faq/#i-noticed-overfitting-issues-in-cycle-1-what-are-your-plans-to-address-this","title":"I noticed overfitting issues in Cycle 1. What are your plans to address this?","text":"<p>Yes, we identified overfitting during Cycle 1. In Cycle 2, while our primary focus is on Time Series models, we also aim to address and mitigate the overfitting issues, time permitting. We're actively researching best practices and fine-tuning techniques to improve model generalization. Remember, this is a learning experience for us too, so we're constantly evolving and adapting to new challenges.</p>"},{"location":"faq/#why-the-shift-to-time-series-models-in-cycle-2","title":"Why the shift to Time Series models in Cycle 2?","text":"<p>Our 'client' expressed an interest in predicting future oil price trends, which is best suited to Time Series models. We're currently researching and developing Time Series models to improve their predictive capabilities.</p>"},{"location":"faq/#will-there-be-further-cycles-after-cycle-2","title":"Will there be further cycles after Cycle 2?","text":"<p>We're currently focused on delivering the best results in Cycle 2, especially with our semester ending in December. Future cycles or iterations will depend on various factors, including project outcomes, feedback, time and evolving objectives.</p>"},{"location":"faq/#how-often-is-the-data-updated","title":"How often is the data updated?","text":"<p>The dataset spans from January 1986 to June 2023 for macroeconomic indicators and until July 2023 for oil prices. However, please note that this data isn't regularly updated, as it primarily serves the educational objectives of our school project.</p>"},{"location":"faq/#can-i-contribute-or-provide-feedback","title":"Can I contribute or provide feedback?","text":"<pre><code>Absolutely! Feedback is invaluable. We welcome contributions via pull requests on our [GitHub repository](). Additionally, for feedback, queries, or issues, you can use the \"Issues\" section on GitHub.\n</code></pre>"},{"location":"references/","title":"References","text":"<ol> <li>G\u00e9ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. GitHub repository | PDF version</li> <li>Polynomial Regression in Python. (n.d.). Retrieved from example.com</li> <li>Anaconda, Inc. (n.d.). Conda documentation. Retrieved from https://docs.conda.io</li> <li>Brownlee, J. (2019). How to Use Learning Curves. Machine Learning Mastery. Retrieved from https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/</li> <li>GeeksforGeeks. (n.d.). GeeksforGeeks. Retrieved from https://www.geeksforgeeks.org/</li> <li>Unit testing in Python code using Pytest + GitHub Actions. Retrieved from YouTube</li> <li>Pandas Documentation. Retrieved from https://pandas.pydata.org/pandas-docs/stable/</li> <li>Jain, A. (2019). Brent Oil Prices Forecast with Prophet and ARIMA. Analytics Vidhya. Retrieved from https://medium.com/analytics-vidhya/brent-oil-prices-forecast-with-prophet-and-arima-50f5f177da5b</li> </ol>"},{"location":"about/changelog/","title":"Changelog","text":""},{"location":"about/changelog/#web-interface-data-exploration-prediction-interface","title":"Web Interface: Data Exploration &amp; Prediction Interface","text":""},{"location":"about/changelog/#version-v020","title":"Version: v0.2.0","text":""},{"location":"about/changelog/#general","title":"General:","text":"<ul> <li> <p>Transitioned into Cycle 2 after successful completion of Cycle 1.</p> </li> <li> <p>Focus shifted towards integration of Time Series models.</p> </li> </ul>"},{"location":"about/changelog/#features","title":"Features:","text":"<ul> <li> <p>Integrated Time Series models for oil price forecasting.</p> </li> <li> <p>Integrated XGBoost, Random Forest, and Polynomial Regression models.</p> </li> </ul>"},{"location":"about/changelog/#bug-fixes-improvements","title":"Bug Fixes &amp; Improvements:","text":"<ul> <li>Enhanced API naming conventions for better readability and standardization.</li> </ul>"},{"location":"about/changelog/#known-issues","title":"Known Issues:","text":""},{"location":"about/changelog/#next-steps","title":"Next Steps:","text":""},{"location":"about/changelog/#version-v010","title":"Version: v0.1.0","text":""},{"location":"about/changelog/#general_1","title":"General:","text":"<ul> <li>Design Web interface - initially made for data exploration and prediction.</li> </ul>"},{"location":"about/changelog/#features_1","title":"Features:","text":"<ul> <li> <p>Added Graph Type Selection: Scatter Plot and Line Plot visualizations.</p> </li> <li> <p>Data Visualization: Implemented interactive charts for better data understanding.</p> </li> <li> <p>Data Filtering: Improved dynamic filtering options.</p> </li> <li> <p>Upcoming Predictions: Integration plans announced; predictive capabilities to be added in the next release.</p> </li> </ul>"},{"location":"about/changelog/#bug-fixes-improvements_1","title":"Bug Fixes &amp; Improvements:","text":"<ul> <li>Fixed an issue with The Oval Table Logo not displaying correctly.</li> </ul>"},{"location":"about/changelog/#known-issues_1","title":"Known Issues:","text":"<ul> <li> <p>Initial Load Time: Due to hosting on Render's free tier, there may be a delay on initial load.</p> </li> <li> <p>Resource Constraints: Memory overflow potential with excessive feature selection.</p> </li> </ul>"},{"location":"about/changelog/#next-steps_1","title":"Next Steps:","text":"<ul> <li> <p>Integrate predictive analytics capabilities.</p> </li> <li> <p>Extend data filtering options.</p> </li> <li> <p>Incorporate more graph types for visualization.</p> </li> </ul>"},{"location":"about/getting-started/","title":"Getting Started with Oil Price Prediction","text":"<p>Welcome to the 'Getting Started' section! Here, we'll guide you through accessing and using our predictive models, either via our web interface or directly from the source code in our GitHub repository.</p>"},{"location":"about/getting-started/#using-the-web-interface","title":"Using the Web Interface","text":"<p>Our user-friendly web interface provides a hassle-free experience to run predictions, view results, and model metrics. Here's how you can get started:</p> <ol> <li> <p>Navigate to the Web Interface: Here</p> </li> <li> <p>Click Predict: On the homepage you will see a button labeled <code>Predict</code>.   This will take you to the prediction page, where you can select a model, either Time Series(for forecasting) or Regression(for regression analysis - these models are not capable of forecasting).</p> </li> <li> <p>Choose a Model: Browse through the available models via the drop-down selection menu and select the one you'd like to use.</p> </li> <li> <p>Select Dataset: Choose from the available datasets via the drop-down menu. We offer three variations to cater to different analysis needs:</p> <ul> <li>Raw Dataset: The untouched, initial dataset as provided to us.</li> <li>... Dataset: Logarithmically transformed to normalize the data, that's it, no other processing.</li> <li>Pruned Dataset: Processed to exclude blank or missing entries.</li> <li>Optimized Dataset: Extensively cleaned, free of negative values, zeros, and blanks.</li> </ul> </li> </ol> <p>Info</p> <p>For a comprehensive overview of our datasets and the preprocessing methods employed, please visit the Data section.</p> <ol> <li> <p>Run the Model: Once you're ready, click on the <code>Predict</code> button.</p> </li> <li> <p>View Results: The results will be displayed in a user-friendly format, showcasing the predictions along with relevant metrics.</p> </li> <li> <p>Deep Dive: For users interested in the nitty-gritty details, each model has an accompanying section detailing its background, methodology, and metrics.</p> </li> </ol>"},{"location":"about/getting-started/#accessing-the-models-via-github","title":"Accessing the Models via GitHub","text":"<p>If you're more hands-on and wish to delve deeper into the code or run the models in your environment, you can access our GitHub repository. Here's how:</p> <ol> <li> <p>Navigate to Our Code Repository: Here</p> </li> <li> <p>Access the Models Directory: In the repository, go to the <code>/Models</code> folder. Here, you'll find directories for each model, e.g., <code>Models/Random-Forest</code>, <code>Models/XGBoost</code>, etc.</p> </li> <li> <p>Explore &amp; Download: Dive into each model's directory to view the source code, any accompanying documentation, and other relevant files. You can clone the repository or download individual files as needed.</p> </li> <li> <p>Access the Data: The data we used for building and training the models is housed under the <code>/Data</code> folder. This ensures you have all necessary components to run the models.</p> </li> <li> <p>Run the Models: The models were developed in a Python 3.11 environment. Depending on your setup, you might need to install certain libraries or dependencies. Refer to the project's README for specific instructions.</p> </li> </ol>"},{"location":"about/getting-started/#docker-support","title":"Docker Support","text":"<p>Info</p> <p>Docker support is planned for the future. Stay tuned for updates!</p>"},{"location":"about/getting-started/#need-assistance","title":"Need Assistance?","text":"<p>If you encounter any issues or have queries, please check out our FAQ section. For more technical problems or feedback, consider raising an issue on our GitHub repository.</p>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0).</p>"},{"location":"about/license/#permissions","title":"Permissions","text":"<p>Under this license, you are permitted to:</p> <ul> <li>Use: You can run and use the software.</li> <li>Modify: You can modify the software.</li> <li>Distribute: You can distribute the software to others.</li> <li>Place Warranty: You can place a warranty on the software.</li> </ul>"},{"location":"about/license/#conditions","title":"Conditions","text":"<p>However, there are certain conditions to keep in mind:</p> <ul> <li> <p>Disclose Source: Source code must be made available when the software is distributed.</p> </li> <li> <p>Same License: Modifications must be released under the same license.</p> </li> <li> <p>State Changes: Changes made to the code must be documented.</p> </li> <li> <p>Network Use is Distribution: Users who interact with the software over a network are given the right to receive the source for that program.</p> </li> </ul>"},{"location":"about/license/#limitations","title":"Limitations","text":"<p>Please be aware of the following limitations:</p> <ul> <li> <p>No Trademark Use: This license does not grant any rights to use the name, logo, or trademarks of the project.</p> </li> <li> <p>No Liability: The software is provided \"as-is\". The project or contributors are not responsible for any damages or issues that might arise from using the software.</p> </li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>We welcome contributions! If you'd like to contribute, please ensure that you follow the terms and conditions of the AGPL-3.0 license. By contributing to this project, you agree to abide by the terms of this license.</p>"},{"location":"about/license/#full-license-text","title":"Full License Text","text":"<p>For the detailed legal terms of the AGPL-3.0 license, please see the official AGPL-3.0 License text.</p>"},{"location":"about/project-requirements/","title":"Project Requirements","text":""},{"location":"about/project-requirements/#client-data-processing-requirements","title":"Client Data Processing Requirements","text":""},{"location":"about/project-requirements/#objective","title":"Objective","text":"<p>The goal is to use macroeconomic indicators to predict the real oil price in the near future, starting with a one-month forecast and potentially extending to 3, 6, and 12 months.</p>"},{"location":"about/project-requirements/#data-transformation-steps","title":"Data Transformation Steps","text":"<ol> <li>Convert Nominal to Real Oil Prices<ul> <li>Nominal Oil Price: The original oil price data provided.</li> <li>CPI (Consumer Price Index): A measure of inflation, found as variable #105 in the macroeconomic data.</li> <li>Real Oil Price Calculation: Adjust the nominal oil price for inflation using the formula: <code>Real Price = Nominal Price / CPI * 100</code>.</li> <li>Purpose: Real prices provide a clearer picture of market dynamics by removing the distortion caused by inflation.</li> </ul> </li> <li> <p>Log Transformation of Macroeconomic Variables</p> <ul> <li>Log Transformation: Apply a logarithmic scale to the macroeconomic variables to stabilize variance and reduce skewness.</li> <li>Exclusions: Do not transform variables that are already in percentage form (e.g., interest rates, unemployment rate) (and of course the 'date' column).</li> </ul> </li> <li> <p>Data Splitting for Model Training and Testing    Initial Split: Use 80% of the data for training the model and 20% for testing.    Alternative Split: Consider a 50/50 split for comparison.</p> </li> <li>Descriptive Statistics    Post-Transformation Analysis: Provide descriptive statistics for the real oil prices to understand their distribution, central tendency, and spread.</li> </ol>"},{"location":"about/project-requirements/#why-log-transform","title":"Why Log Transform?:","text":"<ul> <li>It helps in dealing with large variations in data, making patterns more perceptible and the data more suitable for modeling.</li> <li>Handling Zero Values: Since the logarithm of zero is undefined, we add a tiny constant (1e-6) to all values before transformation. This is a common practice to avoid mathematical errors without significantly altering the data.</li> </ul>"},{"location":"about/project-requirements/#notes-on-data-transformation","title":"Notes on Data Transformation","text":"<ul> <li>Log Base: The base of the logarithm used in transformations is typically base 10 or base e (natural logarithm) in pandas when using np.log.</li> <li>Small Constant Addition: The addition of <code>1e-6</code> is to ensure no log transformation involves a zero value. Given the smallest non-zero values in the dataset are significantly larger than <code>1e-6</code>, this addition will not materially affect the data.</li> <li>Data Integrity: Ensure that all transformations preserve the integrity of the data, maintaining its original meaning and relationships.</li> </ul>"},{"location":"about/release-notes/","title":"Release Notes - The Oval Table","text":""},{"location":"about/release-notes/#the-oval-table-oil-price-prediction-models","title":"The Oval Table - Oil Price Prediction Models","text":""},{"location":"about/release-notes/#version-v10","title":"Version: v1.0","text":""},{"location":"about/release-notes/#cycle-1-08242023-10082023-summary","title":"Cycle 1 (08/24/2023 - 10/08/2023) Summary:","text":""},{"location":"about/release-notes/#introduction-to-ml","title":"Introduction to ML:","text":"<ul> <li> <p>First venture into the realm of Machine Learning.</p> </li> <li> <p>Laid the groundwork with Regression models to predict oil prices.</p> </li> <li> <p>Models Introduced:</p> <ul> <li> <p>XGBoost: Utilized a gradient boosting framework with decision trees.</p> </li> <li> <p>Random Forest: An ensemble learning method utilizing multiple decision trees.</p> </li> <li> <p>Polynomial Regression: A regression algorithm modeling the relationship between input and output as an nth degree polynomial.</p> </li> <li> <p>Baseline Model: Introduced a Naive Forecast as a basic comparison tool.</p> </li> </ul> </li> </ul>"},{"location":"about/release-notes/#challenges-learnings","title":"Challenges &amp; Learnings:","text":"<ul> <li>Overfitting: Identified that models were prone to overfitting on the training data.</li> </ul>"},{"location":"about/release-notes/#next-steps","title":"Next Steps:","text":"<ul> <li>Implement model refinement to address overfitting issues.</li> <li>Integrate time series forecasting to predict future oil prices.</li> <li>Web interface integration.</li> </ul>"},{"location":"about/release-notes/#cycle-2-10092023-present-updates","title":"Cycle 2 (10/09/2023 - Present) Updates:","text":""},{"location":"about/release-notes/#shift-in-focus","title":"Shift in Focus:","text":"<ul> <li> <p>Transitioned the core focus towards Time Series models, as per the wants and requirements of our 'client', (a professor at CSU).</p> </li> <li> <p>A priority has been set on enhancing oil price prediction accuracy using Time Series methods.</p> </li> </ul>"},{"location":"about/release-notes/#time-series-models","title":"Time Series Models:","text":"<ul> <li> <p>Delving into Time Series models to cater to the specific needs of oil price forecasting.</p> </li> <li> <p>Research and development are in progress to refine these models and improve their predictive capabilities.</p> </li> </ul>"},{"location":"about/release-notes/#model-refinement-plans","title":"Model Refinement Plans:","text":"<ul> <li> <p>While Time Series remains the primary focus, there's an underlying aim to address the overfitting issues from Cycle 1.</p> </li> <li> <p>Depending on the progress and time constraints, there might be further refinements made to the regression models from the previous cycle.</p> </li> </ul>"},{"location":"about/release-notes/#data-cleaning-preprocessing","title":"Data Cleaning &amp; Preprocessing:","text":"<ul> <li> <p>Recognized the need for more meticulous data cleaning.</p> </li> <li> <p>Plans are set to further refine preprocessing steps, if the semester's timeline allows.</p> </li> </ul>"},{"location":"data/about-data/","title":"About the Data","text":""},{"location":"data/about-data/#data-overview","title":"Data Overview","text":"<p>The datasets powering our oil price prediction models have been carefully curated, stemming from reliable sources and kindly provided by our sponsor. Our primary data channels revolve around historical oil prices, accompanied by significant macroeconomic indicators that potentially influence oil market dynamics.</p>"},{"location":"data/about-data/#data-sources","title":"Data Sources","text":"<p>Macroeconomic Data.csv: This dataset features macroeconomic indicators spanning from January 1986 to June 2023. This comprehensive collection offers a deeper understanding of economic parameters that may have had an impact on oil price trends over the years.</p> <p>RWTCm.xls: This file houses data specific to the Cushing, OK WTI Spot Price FOB (Dollars per Barrel). It encompasses a timeframe from January 1986 up until July 2023.</p>"},{"location":"data/about-data/#data-integrity","title":"Data Integrity","text":"<p>The datasets have been generously provided by our 'sponsor,' a distinguished professor in the Economics department of our school. While the data is thorough and well-researched, it's crucial to note that our datasets aren't regularly updated, primarily because they serve the educational motives of this academic project.</p>"},{"location":"data/about-data/#data-preprocessing","title":"Data Preprocessing","text":"<p>In ensuring the reliability of our predictions, we followed strict guidelines for data preprocessing. The requirements given to us necessitated certain transformations to ensure the data was in the right state for analysis and modeling. Here are the steps we took:</p> <pre><code>Real Price = Nominal Price/CPI \u00d7 100\n</code></pre> <p>Where:</p> <ul> <li> <p>Nominal Price: The original oil price as provided in the dataset (before inflation adjustment).</p> </li> <li> <p>CPI (Consumer Price Index): This reflects the average change over time in the prices paid by consumers. In our dataset, CPI (All items) is represented by the variable <code>CPIAUCSL</code> in the macroeconomic data. This transformation ensured that the price of oil is adjusted for inflation, offering a clearer picture of the market dynamics.</p> </li> </ul>"},{"location":"data/about-data/#nominal-to-real-oil-price-conversion","title":"Nominal to Real Oil Price Conversion","text":"<p>The provided dataset has the oil price in nominal terms, which isn't adjusted for inflation. To extract more meaningful insights that reflect true market dynamics, we converted the nominal oil prices to real prices using the following formula:</p>"},{"location":"data/about-data/#log-transformation-of-macroeconomic-variables","title":"Log Transformation of Macroeconomic Variables","text":"<p>Due to the significant variation in the macroeconomic data on a relative scale, a log transformation was applied to all the variables, with exceptions:</p> <ul> <li>Variables expressed in percentages, our client instructed us to consider the following 18 variables as percentages:</li> <li><code>UNRATE</code>, <code>FEDFUNDS</code>, <code>CP3Mx</code>, <code>TB3MS</code>, <code>TB6MS</code>, <code>GS1</code>, <code>GS5</code>, <code>GS10</code>, <code>AAA</code>, <code>BAA</code>, <code>COMPAPFF</code>, <code>TB3SMFFM</code>, <code>TB6SMFFM</code>, <code>T1YFFM</code>, <code>T5YFFM</code>, <code>T10YFFM</code>, <code>AAAFFM</code>, <code>BAAFFM</code></li> <li> <p>For more information on what exactly any of these variables represent, please refer to the Macroeconomic Data Dictionary and search accordingly.</p> </li> <li> <p>The log transformation, denoted as log(x), helps in stabilizing variance and making the data more amenable to modeling.</p> </li> </ul>"},{"location":"data/about-data/#what-is-a-log-transformation","title":"What is a Log Transformation?","text":"<ul> <li>A log transformation is a process where you take the logarithm of the values in your dataset. This is often done in data analysis to deal with skewed data or to make patterns more visible.</li> </ul> <p>Info</p> <p>For this project, we used log(x) base 2 initially (Cycle 1) but transitioned to log(x) base e (<code>numpy</code> default) for Cycle 2. The base doesn't matter as long as it's consistent across all variables.</p>"},{"location":"data/about-data/#why-add-a-small-constant-before-log-transformation","title":"Why Add a Small Constant Before Log Transformation?","text":"<ul> <li>You can't take the logarithm of zero because it's undefined. To avoid this problem, you add a very small number to all your values so that none of them are zero anymore.</li> </ul>"},{"location":"data/about-data/#how-small-should-the-constant-be","title":"How Small Should the Constant Be?","text":"<ul> <li>The constant should be smaller than any other value in your dataset. This way, it doesn't change your data much; it just makes sure you don't have any zeros.</li> </ul>"},{"location":"data/about-data/#our-dataset","title":"Our Dataset","text":"<ul> <li>Based on the smallest values we've found in our dataset (after log transformation), which are all larger than 1 (like 11.367264, 11.453453, etc.), adding a tiny number like 0.000001 (which is what 1e-6 is) won't really change those numbers in any meaningful way. It's like adding a drop of water to a swimming pool\u2014it doesn't change the amount of water in the pool in a way you'd notice.</li> </ul>"},{"location":"data/about-data/#in-simple-steps","title":"In Simple Steps:","text":"<ol> <li> <p>Check Your Data: We looked at our data and found the smallest numbers are all much bigger than zero (not including blank values or negative values, which we'll deal with later).</p> </li> <li> <p>Choose a Constant: We've chosen <code>1e-6</code> because it's much smaller than our smallest data point, which in our case was <code>11.367264</code>.</p> </li> <li> <p>Apply the Transformation: We add this tiny number to all our data points so none are zero, and then take the logarithm of all these slightly adjusted numbers.</p> </li> <li> <p>Result: Now all data points have been transformed, and we can use them in our analysis without worrying about the problems that come with taking the logarithm of zero.</p> </li> </ol> <p>So, in this case, using <code>1e-6</code> is perfectly fine because it's so small compared to our actual data that it won't really affect our analysis, but it will allow us to perform the log transformation without any issues.</p>"},{"location":"data/about-data/#dataset-versions","title":"Dataset Versions","text":"<ul> <li> <p>Combined_Raw.csv (Macroeconomic Data + Oil Price from RWTCm.xls): This is the raw dataset of Macroeconomic Data.csv and RWTCm.xls combined. It's the original dataset as provided to us, with no transformations applied or Real Oil Price calculated using the CPI.</p> <ul> <li>Blanks: 90</li> <li>Zeroes: 46</li> <li>Negatives: 1022</li> <li>Rows: 450</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1986-01 - 2023-06</li> <li>Source: See here</li> </ul> </li> <li> <p>Combined_CPI_Adjusted.csv: This is still the raw dataset of Macroeconomic Data.csv and RWTCm.xls combined, but with the CPI adjustment applied to the oil price to get the Real Oil Price.</p> <ul> <li>Blanks: 90</li> <li>Zeroes: 46</li> <li>Negatives: 1022</li> <li>Rows: 450</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1986-01 - 2023-06</li> <li>Source: See here</li> </ul> </li> <li> <p>Combined_Log_Transformed.csv: The dataset with a log transformation applied to all applicable (non-percentage) variables, including real oil prices.</p> <ul> <li>Blanks: 199</li> <li>Zeroes: 36</li> <li>Negatives: 2281</li> <li>Rows: 450</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1986-01 - 2023-06</li> <li>Source: See here</li> </ul> </li> <li> <p>Combined_Log_Clean.csv: This dataset is derived previous dataset <code>Combined_Log_Transformed.csv</code> with all blanks removed.</p> </li> <li>Blanks: 0</li> <li>Zeros: 34</li> <li>Negatives: 1454</li> <li>Rows: 303 (147 rows removed from previous dataset)</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1992-02 - 2023-03</li> <li> <p>Source: See here</p> </li> <li> <p>Combined_Log_Clean_NoNeg.csv: Similar to the previous dataset (<code>Combined_Log_Clean.csv</code>) but with all negative values removed.</p> <ul> <li>Blanks: 0</li> <li>Zeros: 6</li> <li>Negatives: 0</li> <li>Rows: 97 (206 rows removed from previous dataset)</li> <li>Columns: 124 (including date)</li> <li>Variables: 123 (not including date)</li> <li>Dropped Variables: 4</li> <li>Dates: 1992-03 - 2019-02</li> <li>Source: See here</li> </ul> </li> <li> <p>Combined_Log_Transformed_Excl_Roil.csv: This dataset originates from <code>Combined_CPI_Adjusted.csv</code> and has a log transformation applied to all applicable (non-percentage) variables, excluding real oil prices.</p> </li> <li>Blanks: 199</li> <li>Zeroes: 36</li> <li>Negatives: 2281</li> <li>Rows: 405</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1986-01 - 2023-06</li> <li> <p>Source: See here</p> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv: This dataset originates from the previous <code>Combined_Log_Transformed_Excl_Roil.csv</code> with all blanks removed.</p> </li> <li>Blanks: 0</li> <li>Zeroes: 34</li> <li>Negatives: 1454</li> <li>Rows: 303 (147 rows removed from previous dataset)</li> <li>Columns: 128 (including date)</li> <li>Variables: 127 (not including date)</li> <li>Dropped Variables: 0</li> <li>Dates: 1986-01 - 2023-06</li> <li> <p>Source: See here</p> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv: An extension of <code>Combined_Log_Excl_Roil_Clean.csv</code>, it excludes rows with negative values and drops columns with predominantly negative data.</p> </li> <li>Blanks: 0</li> <li>Zeroes: 6</li> <li>Negatives: 0</li> <li>Rows: 97 (206 rows removed from previous dataset)</li> <li>Columns: 124 (including date)</li> <li>Variables: 123 (not including date)</li> <li>Dropped Variables: 4</li> <li>Dates: 1992-03 - 2019-02</li> <li>Source: See here</li> </ul>"},{"location":"data/about-data/#data-usage","title":"Data Usage","text":"<p>It's essential to understand that while our datasets are extensive and informative, they are primarily tailored for educational purposes within the context of our current project. We advise users to approach the data with a discerning lens, acknowledging its scope and potential limitations.</p>"},{"location":"data/model-performance/","title":"Model Performance on Various Datasets","text":"<p>The results provided show the performance of several models on various datasets, with metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and the coefficient of determination (R^2). </p> <p>Here's a breakdown of what these results suggest for each model:</p>"},{"location":"data/model-performance/#naive-forecast-baseline","title":"Naive Forecast (Baseline)","text":"<p>The Naive Forecast model, which predicts future values based on the most recent data point, has been evaluated across various datasets. Here's a breakdown of its performance:</p> <ul> <li> <p>Combined_Raw.csv:</p> <ul> <li>MAE: <code>3.09</code>, MSE: <code>22.14</code>, RMSE: <code>4.71</code>, MAPE: <code>6.99%</code>, R^2: <code>0.975</code></li> <li>This dataset shows a relatively high accuracy with an R^2 of 0.975, indicating the model can explain 97.5% of the variance in the data.</li> </ul> </li> <li> <p>Combined_CPI_Adjusted.csv:</p> <ul> <li>MAE: <code>1.46</code>, MSE: <code>4.42</code>, RMSE: <code>2.10</code>, MAPE: <code>6.89%</code>, R^2: <code>0.963</code></li> <li>The model performs well, with a high R^2 value, suggesting good predictive capability.</li> </ul> </li> <li> <p>Combined_Log_Transformed.csv:</p> <ul> <li>MAE: <code>0.068</code>, MSE: <code>0.009</code>, RMSE: <code>0.097</code>, MAPE: <code>2.36%</code>, R^2: <code>0.956</code></li> <li>Excellent performance with low error metrics and a high R^2, indicating strong predictive accuracy.</li> </ul> </li> <li> <p>Combined_Log_Clean.csv:</p> <ul> <li>MAE: <code>0.068</code>, MSE: <code>0.010</code>, RMSE: <code>0.101</code>, MAPE: <code>2.32%</code>, R^2: <code>0.951</code></li> <li>Similar to the log-transformed dataset, showing high accuracy and low error rates.</li> </ul> </li> <li> <p>Combined_Log_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.087</code>, MSE: <code>0.019</code>, RMSE: <code>0.137</code>, MAPE: <code>2.99%</code>, R^2: <code>0.889</code></li> <li>Slightly lower performance compared to other log-transformed datasets but still shows good predictive capability.</li> </ul> </li> <li> <p>Combined_Log_Transformed_Excl_Roil.csv:</p> <ul> <li>MAE: <code>1.46</code>, MSE: <code>4.42</code>, RMSE: <code>2.10</code>, MAPE: <code>6.89%</code>, R^2: <code>0.963</code></li> <li>Performance is identical to the CPI-adjusted dataset, indicating consistent model behavior across these datasets.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv:</p> <ul> <li>MAE: <code>1.55</code>, MSE: <code>5.93</code>, RMSE: <code>2.44</code>, MAPE: <code>7.04%</code>, R^2: <code>0.949</code></li> <li>Shows good predictive accuracy, though slightly lower than the CPI-adjusted and log-transformed datasets.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv</p> <ul> <li>MAE: <code>1.79</code>, MSE: <code>8.64</code>, RMSE: <code>2.94</code>, MAPE: <code>8.98%</code>, R^2: <code>0.883</code></li> <li>This dataset shows the lowest performance among the evaluated datasets, but still maintains a decent level of predictive accuracy.</li> </ul> </li> </ul>"},{"location":"data/model-performance/#best-dataset","title":"'Best' Dataset:","text":"<p>For the Naive Forecast model, the dataset that yielded the 'best' results is the <code>Combined_Log_Transformed.csv</code>.</p> <p>The Naive Forecast model demonstrates strong performance across various datasets, particularly with log-transformed data, where it achieves high R^2 values and low error metrics.</p> <p>This suggests that the most recent data point is often a good predictor for the next, especially in datasets where logarithmic transformations have been applied, indicating a level of consistency or trend in the data. However, it's important to note that while the Naive Forecast can be surprisingly effective, it's a simple approach that doesn't account for complex patterns or potential changes in trends.</p>"},{"location":"data/model-performance/#random-forest","title":"Random Forest","text":"<ul> <li> <p>Datasets with NaNs: We've chosen not to use Random Forest on the following datasets: <code>Combined_Raw.csv</code>, <code>Combined_CPI_Adjusted.csv</code>, and <code>Combined_Log_Transformed.csv</code> due to containing NaN values.  Random Forest cannot handle NaNs; they need to be imputed or the rows/columns with NaNs must be dropped before modeling.</p> </li> <li> <p>Combined_Log_Clean.csv:</p> <ul> <li>MAE: <code>0.085</code>, MSE: <code>0.016</code>, RMSE: <code>0.126</code>, MAPE: <code>3.01%</code>, R^2: <code>0.92</code></li> <li>The Random Forest model shows strong performance on the cleaned log-transformed dataset. The high R^2 score indicates that the model explains a significant portion of the variance in the data.</li> </ul> </li> <li> <p>Combined_Log_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.043</code>, MSE: <code>0.003</code>, RMSE: <code>0.058</code>, MAPE: <code>1.55%</code>, R^2: <code>0.98</code></li> <li>This dataset yields the best performance for Random Forest, with very low error metrics and a high R^2 score. Removing negative values seems to have a positive impact on the model's accuracy.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv:</p> <ul> <li>MAE: <code>1.70</code>, MSE: <code>5.88</code>, RMSE: <code>2.43</code>, MAPE: <code>9.00%</code>, R^2: <code>0.94</code></li> <li>While the performance is strong, the errors are higher compared to the datasets where the real oil price is included and log-transformed. This suggests that including real oil data helps improve the model's predictions.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.80</code>, MSE: <code>0.95</code>, RMSE: <code>0.98</code>, MAPE: <code>4.25%</code>, R^2: <code>0.99</code></li> <li>This dataset shows excellent performance with the highest R^2 score among the datasets where real oil price is excluded from log transformation. The low error metrics indicate a highly accurate model.</li> </ul> </li> </ul>"},{"location":"data/model-performance/#best-dataset_1","title":"'Best' Dataset:","text":"<p>The Random Forest model performs exceptionally well on datasets that have undergone log transformation and cleaning, especially when negative values are removed. The inclusion of real oil price data, when log-transformed, appears to significantly enhance the model's predictive capabilities.</p> <p>The MAPE values are consistently low across datasets where the model is applied, indicating that the predictions are, on average, very close to the actual values. The high R^2 scores, particularly for the <code>Combined_Log_Clean_NoNeg.csv</code> and <code>Combined_Log_Excl_Roil_Clean_NoNeg.csv</code> datasets, demonstrate the model's effectiveness in capturing the variance in the data.</p> <p>These results underscore the importance of data preprocessing and feature selection in building effective predictive models. The Random Forest algorithm, in particular, benefits from cleaner data (free from NaNs and negative values) and appropriate transformations, leading to more accurate predictions. It's crucial to ensure that these models are not overfitting the training data, which can be verified through cross-validation or by evaluating the model's performance on a separate test set.</p>"},{"location":"data/model-performance/#xgboost","title":"XGBoost","text":"<p>The results provided showcases the performance of the XGBoost regression model across various datasets, evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and the coefficient of determination (R^2). Here's an analysis of the results:</p> <ul> <li> <p>Combined_Raw.csv:</p> <ul> <li>MAE: <code>2.72</code>, MSE: <code>14.92</code>, RMSE: <code>3.86</code>, MAPE: <code>6.41%</code>, R^2: <code>0.98</code></li> <li>This dataset shows excellent performance with a very high R^2 score, indicating that the model explains a significant portion of the variance in the data. The errors are relatively low, suggesting good predictive accuracy.</li> </ul> </li> <li> <p>Combined_CPI_Adjusted.csv:</p> <ul> <li>MAE: <code>1.57</code>, MSE: <code>4.73</code>, RMSE: <code>2.17</code>, MAPE: <code>7.58%</code>, R^2: <code>0.95</code></li> <li>The model performs well on this dataset too, with a high R^2 score. The errors are slightly higher compared to the raw dataset, which might be due to the CPI adjustment affecting the data's characteristics. </li> </ul> </li> <li> <p>Combined_Log_Transformed.csv:</p> <ul> <li>MAE: <code>0.07</code>, MSE: <code>0.01</code>, RMSE: <code>0.10</code>, MAPE: <code>2.53%</code>, R^2: <code>0.94</code></li> <li>The logarithmic transformation seems to have significantly improved the model's performance, with very low error metrics and a high R^2 score. This indicates that the log transformation helps in linearizing the relationships in the data.</li> </ul> </li> <li> <p>Combined_Log_Clean.csv:</p> <ul> <li>MAE: <code>0.08</code>, MSE: <code>0.01</code>, RMSE: <code>0.12</code>, MAPE: <code>2.88%</code>, R^2: <code>0.93</code></li> <li>Cleaning the data post-log transformation maintains strong model performance, although there's a slight increase in errors and a minor decrease in R^2 compared to the non-cleaned log-transformed data.</li> </ul> </li> <li> <p>Combined_Log_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.05</code>, MSE: <code>0.004</code>, RMSE: <code>0.07</code>, MAPE: <code>1.81%</code>, R^2: <code>0.97</code></li> <li>Removing negative values further enhances model performance, resulting in the lowest errors and a very high R^2 score among all datasets. This suggests that negative values might have been outliers or noise in the data.</li> </ul> </li> <li> <p>Combined_Log_Transformed_Excl_Roil.csv:</p> <ul> <li>MAE: <code>1.48</code>, MSE: <code>4.12</code>, RMSE: <code>2.03</code>, MAPE: <code>7.10%</code>, R^2: <code>0.96</code></li> <li>Excluding the real oil price from the log transformation results in increased errors and a slightly lower R^2 score compared to the full log-transformed dataset. This indicates that the real oil price contributes significantly to the model's predictive power.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv:</p> <ul> <li>MAE: <code>1.60</code>, MSE: <code>4.12</code>, RMSE: <code>2.03</code>, MAPE: <code>8.01%</code>, R^2: <code>0.96</code></li> <li>Cleaning the dataset while excluding real oil price from log transformation shows similar performance to the non-cleaned version, with a slight increase in MAPE.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>1.14</code>, MSE: <code>2.04</code>, RMSE: <code>1.43</code>, MAPE: <code>6.18%</code>, R^2: <code>0.97</code></li> <li>This dataset, after excluding the real oil price from the log transformation and cleaning negative values, shows impressive performance. The R^2 score of <code>0.97</code> indicates that the model explains a vast majority of the variance in the dataset. The errors (MAE, MSE, RMSE) are relatively low, and the MAPE of <code>6.18%</code> suggests that the predictions are, on average, within about <code>6.18%</code> of the actual values, which is quite accurate for forecasting tasks.</li> </ul> </li> </ul>"},{"location":"data/model-performance/#best-dataset_2","title":"'Best' Dataset:","text":"<p>The <code>Combined_Log_Clean_NoNeg.csv</code> dataset generally shows the best performance across most metrics, highlighting the effectiveness of log transformation and cleaning negative values in the data.</p> <p>The inclusion of the real oil price in the log transformation appears to be crucial for model performance, as seen in the datasets where it's excluded.</p>"},{"location":"data/model-performance/#polynomial-regression","title":"Polynomial Regression","text":"<ul> <li> <p>Datasets with NaNs: We've chosen not to use Polynomial Regression on the following datasets: <code>Combined_Raw.csv</code>, <code>Combined_CPI_Adjusted.csv</code>, and <code>Combined_Log_Transformed.csv</code> due to containing NaN values.  Polynomial Regression cannot handle NaNs; they need to be imputed or the rows/columns with NaNs must be dropped before modeling.</p> </li> <li> <p>Combined_Log_Clean.csv:</p> <ul> <li>MAE: <code>0.10</code>, MSE: <code>0.017</code>, RMSE: <code>0.129</code>, MAPE: <code>3.44%</code>, R^2: <code>0.92</code></li> <li>This dataset, with log transformation and cleaning, shows strong performance in the polynomial regression model. The R^2 score of <code>0.92</code> is quite high, indicating that the model explains a significant portion of the variance in the data. The error metrics are low, and a MAPE of <code>3.44%</code> suggests good accuracy.</li> </ul> </li> <li> <p>Combined_Log_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.094</code>, MSE: <code>0.014</code>, RMSE: <code>0.119</code>, MAPE: <code>3.30%</code>, R^2: <code>0.92</code></li> <li>Similar to the previous dataset, this one also shows strong performance. The removal of negative values seems to have a slightly positive impact on the model's accuracy, as indicated by the marginally lower error metrics and MAPE.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv:</p> <ul> <li>MAE: <code>2.43</code>, MSE: <code>11.78</code>, RMSE: <code>3.43</code>, MAPE: <code>14.26%</code>, R^2: <code>0.89</code></li> <li>In this dataset, where the real oil price is included but not log-transformed, the model's performance drops compared to the previous two datasets. The errors are significantly higher, and the R^2 score, while still good, is lower. This suggests that the log transformation of the real oil price is beneficial for the model's performance.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>1.73</code>, MSE: <code>5.03</code>, RMSE: <code>2.24</code>, MAPE: <code>10.01%</code>, R^2: <code>0.93</code></li> <li>This dataset, which excludes the real oil price from log transformation and cleans negative values, shows improved performance compared to the previous dataset but is still not as strong as when the real oil price is log-transformed. The R^2 score is high, but the error metrics, especially MAPE, are higher than in the first two datasets.</li> </ul> </li> </ul>"},{"location":"data/model-performance/#best-dataset_3","title":"'Best' Dataset:","text":"<p>The <code>Combined_Log_Clean.csv</code> and <code>Combined_Log_Clean_NoNeg.csv</code> datasets yield the best results with the Polynomial Regression model, indicating that log transformation and cleaning of the data are crucial for model performance.</p> <p>Excluding the real oil price from log transformation (while still including it in the dataset) leads to a decrease in model performance, suggesting that the way this feature is processed has a significant impact. The relatively high R^2 scores across all datasets indicate that Polynomial Regression is a suitable model for this type of data, but the choice of data preprocessing steps (like log transformation and cleaning of negatives) plays a crucial role in achieving optimal performance.</p> <p>As with any model, it's important to validate these results on unseen data to ensure that the model generalizes well and is not overfitting.</p>"},{"location":"data/model-performance/#arima","title":"ARIMA","text":"<ul> <li> <p>Combined_Raw.csv:</p> <ul> <li>MAE: <code>16.80</code>, MSE: <code>522.13</code>, RMSE: <code>22.85</code>, MAPE: <code>25.48%</code>, R^2: <code>-0.49</code></li> <li>The ARIMA model performs poorly on the raw dataset, as indicated by high error metrics and a negative R^2 score. This suggests that the model is not suitable for the raw data without preprocessing.</li> </ul> </li> <li> <p>Combined_CPI_Adjusted.csv:</p> <ul> <li>MAE: <code>4.47</code>, MSE: <code>36.84</code>, RMSE: <code>6.07</code>, MAPE: <code>20.41%</code>, R^2: <code>-0.10</code></li> <li>Although the error metrics are lower compared to the raw dataset, the negative R^2 score still indicates poor model performance. This suggests that CPI adjustment alone is not sufficient for the ARIMA model to perform well.</li> </ul> </li> <li> <p>Combined_Log_Transformed.csv:</p> <ul> <li>MAE: <code>0.21</code>, MSE: <code>0.081</code>, RMSE: <code>0.285</code>, MAPE: <code>7.06%</code>, R^2: <code>-0.05</code></li> <li>The log transformation improves the model's performance significantly, as seen in the lower error metrics. However, the slightly negative R^2 score suggests that the model still does not fit the data well.</li> </ul> </li> <li> <p>Combined_Log_Clean.csv:</p> <ul> <li>MAE: <code>0.28</code>, MSE: <code>0.106</code>, RMSE: <code>0.326</code>, MAPE: <code>8.76%</code>, R^2: <code>-0.58</code></li> <li>Despite cleaning the data, the ARIMA model's performance does not improve significantly. The negative R^2 score indicates a poor fit to the data.</li> </ul> </li> <li> <p>Combined_Log_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>0.23</code>, MSE: <code>0.112</code>, RMSE: <code>0.334</code>, MAPE: <code>7.62%</code>, R^2: <code>-0.22</code></li> <li>Removing negative values does not lead to a substantial improvement in model performance. The negative R^2 score persists, indicating a poor fit.</li> </ul> </li> <li> <p>Combined_Log_Transformed_Excl_Roil.csv:</p> <ul> <li>MAE: <code>4.47</code>, MSE: <code>36.84</code>, RMSE: <code>6.07</code>, MAPE: <code>20.41%</code>, R^2: <code>-0.10</code></li> <li>Similar to the Combined_CPI_Adjusted.csv dataset, the performance here is poor, with a negative R^2 score, suggesting that excluding real oil price from log transformation does not benefit the ARIMA model.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv:</p> <ul> <li>MAE: <code>6.27</code>, MSE: <code>58.57</code>, RMSE: <code>7.65</code>, MAPE: <code>24.66%</code>, R^2: <code>-0.68</code></li> <li>This dataset shows one of the worst performances, with high error metrics and a significantly negative R^2 score, indicating a very poor fit.</li> </ul> </li> <li> <p>Combined_Log_Excl_Roil_Clean_NoNeg.csv:</p> <ul> <li>MAE: <code>6.19</code>, MSE: <code>69.51</code>, RMSE: <code>8.34</code>, MAPE: <code>29.55%</code>, R^2: <code>-0.17</code></li> <li>Despite cleaning and removing negative values, the performance remains poor, as indicated by high error metrics and a negative R^2 score.</li> </ul> </li> </ul>"},{"location":"data/model-performance/#best-dataset_4","title":"'Best' Dataset:","text":"<p>The ARIMA model generally performs poorly across all datasets, with negative R^2 scores in most cases, indicating that it is not a suitable model for this type of data.</p> <p>The log-transformed datasets show relatively better performance compared to others, but the improvement is not enough to make the ARIMA model competitive with other models like XGBoost or Polynomial Regression.</p> <p>The consistently negative R^2 scores across different datasets suggest that ARIMA struggles to capture the underlying patterns in this data, possibly due to its non-stationary nature or complex relationships that ARIMA cannot model effectively. These results highlight the importance of choosing the right model for the data at hand and the limitations of ARIMA in handling complex datasets like these.</p>"},{"location":"data/model-performance/#why-arima-may-not-be-suitable-for-this-data","title":"Why  ARIMA may not be suitable for this data?","text":"<p>ARIMA (AutoRegressive Integrated Moving Average) models might not perform well on certain datasets due to a few key reasons:</p> <ol> <li> <p>Non-Stationary Data: ARIMA models are designed for stationary time series, where the statistical properties like mean and variance are constant over time. If the data shows trends or seasonality, it needs to be transformed into a stationary form before using ARIMA, which might not always be effective.</p> </li> <li> <p>Complex Relationships: ARIMA models may struggle with data that has complex, non-linear relationships. They are fundamentally linear models and might not capture complex patterns present in some datasets, especially those influenced by numerous external factors.</p> </li> <li> <p>Lack of External Factors: ARIMA models primarily focus on the time series data itself and do not incorporate external variables or factors. If the dataset is influenced by external factors (like economic indicators, global events, etc.), ARIMA might not be able to account for these influences.</p> </li> <li> <p>Overfitting on Noisy Data: ARIMA can overfit on noisy data, leading to a model that performs well on training data but poorly on unseen data. This is particularly problematic if the noise in the data is random and does not contain information about future values.</p> </li> <li> <p>Parameter Selection: Choosing the right parameters (p, d, q) for ARIMA models can be challenging. Incorrect parameter choices can lead to poor model performance.</p> </li> </ol> <p>In summary, ARIMA's limitations in handling non-stationary data, complex relationships, external factors, and its sensitivity to parameter selection and noisy data can contribute to its suboptimal performance on certain datasets.</p>"},{"location":"data/model-performance/#prophet","title":"Prophet","text":"<ul> <li> <p>Combined_Raw.csv: This dataset contains the raw data without any preprocessing. The model's performance is moderate with an R^2 of <code>0.72</code>. However, the error metrics like MAE and RMSE are relatively high, suggesting that the raw data might have outliers or noise affecting the model's predictions.</p> </li> <li> <p>Combined_CPI_Adjusted.csv: Adjusting for CPI helps in normalizing the data related to inflation or other economic changes over time. The model shows a decent fit with an R^2 of <code>0.66</code>, but the error metrics are still on the higher side.</p> </li> <li> <p>Combined_Log_Transformed.csv: Log transformation helps in stabilizing the variance and making the data more 'model-friendly'. This dataset shows significant improvement in all metrics, especially in reducing the MAPE to <code>6.19%</code> and achieving an R^2 of <code>0.73.</code></p> </li> <li> <p>Combined_Log_Clean.csv: Further cleaning the log-transformed data results in slightly better performance, indicating that removing anomalies or irrelevant data points can enhance model accuracy.</p> </li> <li> <p>Combined_Log_Clean_NoNeg.csv: This dataset yields the best performance across all metrics, with the lowest errors and the highest R^2 of <code>0.77</code>. Removing negative values seems to have a substantial positive impact, suggesting that such values might have been outliers or noise.</p> </li> <li> <p>Combined_Log_Transformed_Excl_Roil.csv: Excluding real oil data in the log-transformed dataset shows a decrease in model performance compared to when real oil data is included, indicating that real oil prices might be a significant predictor for the model.</p> </li> <li> <p>Combined_Log_Excl_Roil_Clean.csv and Combined_Log_Excl_Roil_Clean_NoNeg.csv: These datasets, which exclude real oil data and involve cleaning, show lower performance compared to datasets where real oil data is included. This further emphasizes the importance of real oil prices in the prediction model.</p> </li> </ul>"},{"location":"data/model-performance/#best-dataset_5","title":"'Best' Dataset:","text":"<p>The <code>Combined_Log_Clean_NoNeg.csv</code> dataset stands out as the 'best' performer for the Prophet model. The combination of log transformation, data cleaning, and removal of negative values significantly enhances the model's predictive accuracy. The low MAPE of <code>5.71%</code> and the highest R^2 of <code>0.77</code> suggest that the model's predictions are both accurate and reliable for this dataset.</p> <p>While the <code>Combined_Log_Clean_NoNeg.csv</code> dataset shows the best performance, it's essential to ensure the model's ability to generalize. This can be done by evaluating the model on a separate test set or using time-series cross-validation techniques.</p>"},{"location":"data/model-performance/#xgboost-time-series","title":"XGBoost (Time Series)","text":"<p>To be added.</p>"},{"location":"data/variables/","title":"Variables","text":""},{"location":"data/variables/#features","title":"Features","text":"<p>The dataset includes a variety of economic, financial, and industrial indicators. Here's a detailed look at each feature [All 127 of them]:</p>"},{"location":"data/variables/#economic-indicators","title":"Economic Indicators","text":"<ol> <li>RPI (Real Personal Income): The total income received by individuals, adjusted for inflation, reflecting the purchasing power of the population.</li> <li>W875RX1 (Real personal income excluding transfers): Personal income excluding government transfers like social security, reflecting income from productive activities.</li> <li>DPCERA3M086SBEA (Real personal consumption expenditures: durable goods): The value of consumer spending on durable goods, adjusted for inflation.</li> <li>CMRMTSPLx (Real Manufacturing and Trade Industries Sales): Sales figures from manufacturing and trade industries, adjusted for inflation.</li> <li>RETAILx (Retail and Food Services Sales): Total sales for retail and food service sectors, indicating consumer spending patterns.</li> <li>INDPRO (Industrial Production Index): A measure of output from the industrial sector, including manufacturing, mining, and utilities.</li> <li>IPFPNSS (Industrial Production: Final Products and Nonindustrial Supplies): Industrial production focused on final products and non-industrial supplies.</li> <li>IPFINAL (Industrial Production: Final Products (Market Group)): Industrial production of final products intended for the marketplace.</li> <li>IPCONGD (Industrial Production: Consumer Goods): The output of consumer goods, a subset of the overall industrial production.</li> <li>IPDCONGD (Industrial Production: Durable Consumer Goods): The production output of consumer durable goods, items expected to last three years or more.</li> <li>IPNCONGD (Industrial Production: Nondurable Consumer Goods): The production output of consumer nondurable goods, items with a short life span.</li> <li>IPBUSEQ (Industrial Production: Business Equipment): The output of business equipment, reflecting business investment in capital goods.</li> <li>IPMAT (Industrial Production: Materials): The production of materials used in the manufacturing process of goods.</li> <li>IPDMAT (Industrial Production: Durable Materials): The production of durable materials that are used in the manufacturing of durable goods.</li> <li>IPNMAT (Industrial Production: Nondurable Materials): The production of nondurable materials used in the manufacturing of nondurable goods.</li> <li>IPMANSICS (Industrial Production: Manufacturing (NAICS)): Industrial production figures specifically for the manufacturing sector, based on the North American Industry Classification System (NAICS).</li> <li>IPB51222S (Industrial Production: Residential utilities): Production of utilities for residential use.</li> <li>IPFUELS (Industrial Production: Fuels): The total output of fuel products.</li> <li>CUMFNS (Capacity Utilization: Manufacturing): The percentage of manufacturing capacity that is being utilized.</li> <li>HWI (Help-Wanted Index for United States): An index measuring the volume of job advertisements, indicating labor market conditions.</li> <li>HWIURATIO (Ratio of Help-Wanted/Unemployment): The ratio of help-wanted ads to the number of unemployed, indicating labor market tightness.</li> <li>CLF16OV (Civilian Labor Force): The total number of people, 16 years and older, who are actively seeking employment.</li> <li>CE16OV (Civilian Employment): The total number of people, 16 years and older, who are currently employed.</li> <li>UNRATE (Unemployment Rate): The percentage of the labor force that is unemployed and actively seeking employment.</li> <li>UEMPMEAN (Average Duration of Unemployment): The average number of weeks that individuals remain unemployed.</li> <li>UEMPLT5 (Number of Unemployed for Less Than 5 Weeks): The count of individuals who have been unemployed for less than five weeks.</li> <li>UEMP5TO14 (Number of Unemployed for 5-14 Weeks): The count of individuals who have been unemployed for between five and fourteen weeks.</li> <li>UEMP15OV (Number of Unemployed for 15 Weeks and Over): The count of individuals who have been unemployed for fifteen weeks or longer.</li> <li>UEMP15T26 (Number of Unemployed for 15-26 Weeks): The count of individuals who have been unemployed for between fifteen and twenty-six weeks.</li> <li>UEMP27OV (Number of Unemployed for 27 Weeks and Over): The count of individuals who have been unemployed for twenty-seven weeks or longer.</li> <li>CLAIMSx (Initial Jobless Claims): The number of people filing new claims for unemployment insurance benefits.</li> <li>PAYEMS (All Employees: Total nonfarm): The total number of nonfarm employees, indicating the level of employment in the economy.</li> <li>USGOOD (All Employees: Goods-producing industries): The number of employees in goods-producing industries such as manufacturing and construction.</li> <li>CES1021000001 (All Employees: Mining and logging): The number of employees in the mining and logging sector.</li> <li>USCONS (All Employees: Construction): The number of employees working in the construction industry.</li> <li>MANEMP (All Employees: Manufacturing): The total number of employees in the manufacturing sector.</li> <li>DMANEMP (All Employees: Durable goods manufacturing): The number of employees in the manufacturing of durable goods.</li> <li>NDMANEMP (All Employees: Nondurable goods manufacturing): The number of employees in the manufacturing of nondurable goods.</li> <li>SRVPRD (All Employees: Service-providing industries): The number of employees in service-providing industries such as education and health services.</li> <li>USTPU (All Employees: Trade, transportation, and utilities): The number of employees in trade, transportation, and utilities sectors.</li> <li>USWTRADE (All Employees: Wholesale trade): The number of employees in the wholesale trade sector.</li> <li>USTRADE (All Employees: Retail trade): The number of employees in the retail trade sector.</li> <li>USFIRE (All Employees: Financial activities): The number of employees in financial activities, including finance, insurance, and real estate.</li> <li>USGOVT (All Employees: Government): The number of individuals employed by federal, state, and local governments.</li> <li>CES0600000007 (Average Weekly Hours of All Employees: Goods-producing): The average number of hours worked per week by employees in goods-producing industries.</li> <li>AWOTMAN (Average Weekly Overtime Hours of All Employees: Manufacturing): The average number of overtime hours worked per week by manufacturing employees.</li> <li>AWHMAN (Average Weekly Hours of All Employees: Manufacturing): The average number of hours worked per week by employees in the manufacturing sector.</li> <li>HOUST (Housing Starts: Total New Privately Owned): The number of new housing units that began construction, indicating the health of the residential construction sector.</li> <li>HOUSTNE (Housing Starts: Northeast): The number of new housing units that began construction in the Northeast region of the United States.</li> <li>HOUSTMW (Housing Starts: Midwest): The number of new housing units that began construction in the Midwest region of the United States.</li> <li>HOUSTS (Housing Starts: South): The number of new housing units that began construction in the South region of the United States.</li> <li>HOUSTW (Housing Starts: West): The number of new housing units that began construction in the West region of the United States.</li> <li>PERMIT (Building Permits: Total New Privately Owned): The number of new building permits issued, a leading indicator for the housing market.</li> <li>PERMITNE (Building Permits: Northeast): The number of new building permits issued in the Northeast region of the United States.</li> <li>PERMITMW (Building Permits: Midwest): The number of new building permits issued in the Midwest region of the United States.</li> <li>PERMITS (Building Permits: South): The number of new building permits issued in the South region of the United States.</li> <li>PERMITW (Building Permits: West): The number of new building permits issued in the West region of the United States.</li> <li>ACOGNO (New Orders for Consumer Goods): The value of new orders placed for consumer goods, indicating future production demand.</li> <li>AMDMNOx (New Orders for Durable Goods): The value of new orders placed for durable goods, items expected to last at least three years.</li> <li>ANDENOx (New Orders for Nondurable Goods): The value of new orders placed for nondurable goods, items with a short life span.</li> <li>AMDMUOx (Unfilled Orders for Durable Goods): The value of orders received by manufacturers for durable goods that have not yet been shipped.</li> <li>BUSINVx (Total Business Inventories): The total value of inventories held by manufacturers, wholesalers, and retailers.</li> <li>ISRATIOx (Total Business: Inventory/Sales Ratio): The ratio of business inventories to sales, indicating the balance between supply and demand.</li> <li>M1SL (M1 Money Stock): The total amount of M1 money supply, including cash and checking deposits.</li> <li>M2SL (M2 Money Stock): The total amount of M2 money supply, including M1 plus savings deposits, small-denomination time deposits, and retail money market mutual fund shares.</li> <li>M2REAL (Real M2 Money Stock): The total real value of M2 money stock, adjusted for inflation.</li> <li>BOGMBASE (Monetary Base; Total): The total monetary base including currency in circulation and reserves held by the Federal Reserve.</li> <li>TOTRESNS (Total Reserves of Depository Institutions): The total reserves held by depository institutions at the Federal Reserve.</li> <li>NONBORRES (Nonborrowed Reserves of Depository Institutions): The reserves depository institutions hold at the Federal Reserve not obtained through borrowing.</li> <li>BUSLOANS (Commercial and Industrial Loans): The total value of commercial and industrial loans at all commercial banks.</li> <li>REALLN (Real Estate Loans at All Commercial Banks): The total value of loans secured by real estate at all commercial banks.</li> <li>NONREVSL (Total Nonrevolving Credit): The total amount of nonrevolving credit extended to individuals and corporations.</li> <li>CONSPI (Consumer Sentiment Index): A measure of the overall economic optimism of consumers based on their saving and spending activity.</li> <li>S&amp;P 500: A stock market index that measures the stock performance of 500 large companies listed on stock exchanges in the United States.</li> <li>S&amp;P: indust (S&amp;P 500 Industrial Sector): A sectoral index of the S&amp;P 500, measuring the performance of the industrial sector within the larger index.</li> <li>S&amp;P div yield (S&amp;P 500 Dividend Yield): The dividend yield of the S&amp;P 500, indicating the ratio of dividends paid to the price of the index.</li> <li>S&amp;P PE ratio (S&amp;P 500 Price to Earnings Ratio): A valuation measure comparing the S&amp;P 500's current market price to its per-share earnings.</li> <li>FEDFUNDS (Federal Funds Effective Rate): The interest rate at which depository institutions trade federal funds with each other overnight.</li> <li>CP3Mx (3-Month Commercial Paper Rate): The interest rate payable on unsecured promissory notes with a fixed maturity of three months.</li> <li>TB3MS (3-Month Treasury Bill Rate): The interest rate on a three-month U.S. Treasury bill, a short-term debt obligation issued by the U.S. Treasury.</li> <li>TB6MS (6-Month Treasury Bill Rate): The interest rate on a six-month U.S. Treasury bill.</li> <li>GS1 (1-Year Treasury Constant Maturity Rate): The yield received for investing in a U.S. government issued treasury security that has a maturity of one year.</li> <li>GS5 (5-Year Treasury Constant Maturity Rate): The yield on a U.S. government debt obligation with a maturity of five years.</li> <li>GS10 (10-Year Treasury Constant Maturity Rate): The yield on a U.S. government debt obligation with a maturity of ten years.</li> <li>AAA (Moody's Seasoned Aaa Corporate Bond Yield): The yield on corporate bonds that are rated Aaa by Moody's, indicating high credit quality.</li> <li>BAA (Moody's Seasoned Baa Corporate Bond Yield): The yield on corporate bonds that are rated Baa by Moody's, indicating moderate credit risk.</li> <li>COMPAPFFx (3-Month Commercial Paper Minus Federal Funds Rate): The spread between the three-month commercial paper rate and the federal funds rate.</li> <li>TB3SMFFM (3-Month Treasury Bill Minus Federal Funds Rate): The spread between the three-month Treasury bill rate and the federal funds rate.</li> <li>TB6SMFFM (6-Month Treasury Bill Minus Federal Funds Rate): The spread between the six-month Treasury bill rate and the federal funds rate.</li> <li>T1YFFM (1-Year Treasury Constant Maturity Minus Federal Funds Rate): The spread between the one-year Treasury constant maturity rate and the federal funds rate.</li> <li>T5YFFM (5-Year Treasury Constant Maturity Minus Federal Funds Rate): The spread between the five-year Treasury constant maturity rate and the federal funds rate.</li> <li>T10YFFM (10-Year Treasury Constant Maturity Minus Federal Funds Rate): The spread between the ten-year Treasury constant maturity rate and the federal funds rate.</li> <li>AAAFFM (Moody's Seasoned Aaa Corporate Bond Minus Federal Funds Rate): The spread between the yield on Aaa corporate bonds and the federal funds rate.</li> <li>BAAFFM (Moody's Seasoned Baa Corporate Bond Minus Federal Funds Rate): The spread between the yield on Baa corporate bonds and the federal funds rate.</li> <li>TWEXAFEGSMTHx (Trade Weighted U.S. Dollar Index: Advanced Foreign Economies, Goods and Services): An index that measures the value of the U.S. dollar relative to the currencies of U.S. major trading partners.</li> <li>EXSZUSx (Switzerland / U.S. Foreign Exchange Rate): The exchange rate between the Swiss Franc and the U.S. dollar.</li> <li>EXJPUSx (Japan / U.S. Foreign Exchange Rate): The exchange rate between the Japanese Yen and the U.S. dollar.</li> <li>EXUSUKx (U.K. / U.S. Foreign Exchange Rate): The exchange rate between the British Pound and the U.S. dollar.</li> <li>EXCAUSx (Canada / U.S. Foreign Exchange Rate): The exchange rate between the Canadian Dollar and the U.S. dollar.</li> <li>WPSFD49207 (Producer Price Index: Finished Goods): An index that measures the average change over time in the selling prices received by domestic producers for their output.</li> <li>WPSFD49502 (Producer Price Index: Finished Consumer Goods): An index that measures the average change over time in the selling prices of consumer goods.</li> <li>WPSID61 (Producer Price Index: Intermediate Materials, Supplies, and Components): An index that measures the average change over time in the selling prices of intermediate goods used in production.</li> <li>WPSID62 (Producer Price Index: Intermediate Materials: Supplies &amp; Components): Another measure of the price changes for intermediate goods used in production.</li> <li>PPICMM (Producer Price Index: Commodities): An index that measures the average change over time in the selling prices of commodities.</li> <li>CPIAUCSL (Consumer Price Index for All Urban Consumers: All Items): An index that measures the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services.</li> <li>CPIAPPSL (Consumer Price Index for All Urban Consumers: Apparel): An index that measures the average change over time in the prices paid by urban consumers for apparel.</li> <li>CPITRNSL (Consumer Price Index for All Urban Consumers: Transportation): An index that measures the average change over time in the prices paid by urban consumers for transportation.</li> <li>CPIMEDSL (Consumer Price Index for All Urban Consumers: Medical Care): An index that measures the average change over time in the prices paid by urban consumers for medical care.</li> <li>CUSR0000SAC (Consumer Price Index for All Urban Consumers: Commodities): An index that measures the average change over time in the prices paid by urban consumers for commodities.</li> <li>CUSR0000SAD (Consumer Price Index for All Urban Consumers: Durables): An index that measures the average change over time in the prices paid by urban consumers for durable goods.</li> <li>CUSR0000SAS (Consumer Price Index for All Urban Consumers: Services): An index that measures the average change over time in the prices paid by urban consumers for services.</li> <li>CPIULFSL (Consumer Price Index for All Urban Consumers: All Items Less Food and Energy): An index that measures the core inflation rate, excluding the volatile food and energy prices.</li> <li>CUSR0000SA0L2 (Consumer Price Index for All Urban Consumers: All Items Less Shelter): An index that measures the inflation rate, excluding the cost of shelter.</li> <li>CUSR0000SA0L5 (Consumer Price Index for All Urban Consumers: All Items Less Medical Care): An index that measures the inflation rate, excluding the cost of medical care.</li> <li>PCEPI (Personal Consumption Expenditures Price Index): An index that measures the average change over time in the prices paid by consumers for goods and services.</li> <li>DDURRG3M086SBEA (Real Personal Consumption Expenditures: Durable Goods): The value of durable goods purchased by individuals, adjusted for inflation.</li> <li>DNDGRG3M086SBEA (Real Personal Consumption Expenditures: Nondurable Goods): The value of nondurable goods purchased by individuals, adjusted for inflation.</li> <li>DSERRG3M086SBEA (Real Personal Consumption Expenditures: Services): The value of services purchased by individuals, adjusted for inflation.</li> <li>CES0600000008 (Average Hourly Earnings of Production and Nonsupervisory Employees: Goods-Producing): The average hourly earnings of employees in the goods-producing sector.</li> <li>CES2000000008 (Average Hourly Earnings of Production and Nonsupervisory Employees: Construction): The average hourly earnings of employees in the construction sector.</li> <li>CES3000000008 (Average Hourly Earnings of Production and Nonsupervisory Employees: Manufacturing): The average hourly earnings of employees in the manufacturing sector.</li> <li>UMCSENTx (University of Michigan: Consumer Sentiment): A survey of personal consumer confidence in economic activity.</li> <li>DTCOLNVHFNM (Total Consumer Loans and Leases Outstanding): The total amount of consumer loans and leases outstanding, indicating the level of consumer debt.</li> <li>DTCTHFNM (Total Consumer Loans and Leases Outstanding: Revolving Credit): The total amount of revolving credit in consumer loans and leases, indicating the level of consumer credit card debt.</li> <li>INVEST (Securities in Bank Credit at All Commercial Banks): The total value of securities held by commercial banks, indicating the level of investment within the banking sector.</li> <li>VIXCLSx (CBOE Volatility Index: VIX): A measure of market risk and investors' expectations of future volatility.</li> <li>Real Oil Prices: The inflation-adjusted price of oil, reflecting the real purchasing power cost of oil over time.</li> </ol>"},{"location":"data/variables/#conclusion","title":"Conclusion","text":"<p>This comprehensive dataset contains a wide variety of indicators across economic, labor, housing, financial markets, and other miscellaneous categories. Each feature provides unique insights, contributing to a holistic understanding of the U.S. economic landscape. Analyzing this dataset could offer valuable perspectives for economists, policy makers, investors, and researchers.</p>"},{"location":"models/arima/","title":"ARIMA Model","text":"<p>ARIMA stands for AutoRegressive Integrated Moving Average. It is a popular statistical method for time series forecasting.</p>"},{"location":"models/arima/#simple-explanation","title":"Simple Explanation","text":"<p>Think of ARIMA as a weather forecaster that looks at patterns of pressure, temperature, and wind speed (past values) to predict the weather (future values). In time series language, ARIMA considers past values (auto-regressive part), trends and seasonality (integrated part), and random shocks (moving average part) to forecast future data points.</p>"},{"location":"models/arima/#formula","title":"Formula","text":"<p>The ARIMA model is represented by three terms: AR(p), I(d), and MA(q), where:</p> <ul> <li>AR(p) is the auto-regressive part with <code>p</code> indicating the number of lag observations included in the model.</li> <li>I(d) is the integrated part, where <code>d</code> is the order of differencing required to make the time series stationary.</li> <li>MA(q) is the moving average part with <code>q</code> indicating the size of the moving average window.</li> </ul> <p>The ARIMA model can be formulated as:</p> <pre><code>(1 - \u03a3[\u03b1\u209a * B^p]) (1 - B)^d Y\u209c = (1 + \u03a3[\u03b8\u2091 * B^q]) \u03b5\u209c\n</code></pre> <p>where:</p> <ul> <li><code>Y\u209c</code> is the time series.</li> <li><code>B</code> is the backshift operator.</li> <li><code>\u03b1\u209a</code> are the parameters for the AR part.</li> <li><code>\u03b8\u2091</code> are the parameters for the MA part.</li> <li><code>\u03b5\u209c</code> is white noise.</li> </ul>"},{"location":"models/arima/#parameters","title":"Parameters","text":"<ul> <li><code>p</code>: The number of lag observations in the model.</li> <li><code>d</code>: The degree of differencing.</li> <li><code>q</code>: The size of the moving average window.</li> </ul>"},{"location":"models/arima/#feature-importance","title":"Feature Importance","text":"<p>In ARIMA, feature importance isn't discussed in the same way as it is for algorithms like decision trees. Instead, the focus is on the significance of the parameters (<code>\u03b1\u209a</code>, <code>\u03b8\u2091</code>) and the lagged values of the time series.</p>"},{"location":"models/arima/#code","title":"Code","text":"<p>To see how we implemented ARIMA in our project including cross-validation, check out the ARIMA notebook.</p>"},{"location":"models/arima/#additional-notes","title":"Additional Notes","text":"<ul> <li>ARIMA models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting).</li> <li>Seasonal ARIMA (SARIMA) is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.</li> </ul> <p>To learn more about ARIMA and its applications, check out the ARIMA documentation.</p>"},{"location":"models/cross-validation/","title":"Cross Validation","text":""},{"location":"models/cross-validation/#what-is-cross-validation","title":"What is Cross-Validation?","text":"<p>Cross-Validation is a statistical method used to estimate the skill of a machine learning model. It is primarily used to assess how well a model will perform on an independent dataset, i.e., data it hasn't seen before. This is crucial for understanding how well your model generalizes from the training data to unseen data.</p>"},{"location":"models/cross-validation/#why-use-cross-validation","title":"Why Use Cross-Validation?","text":"<p>When you build a machine learning model, you typically divide your data into at least two sets: a training set and a test set.  The model learns from the training set and is evaluated on the test set. However, this split can lead to variability in the model's performance depending on which data points end up in the training set and which in the test set. Cross-Validation helps to mitigate this by using multiple splits.</p>"},{"location":"models/cross-validation/#what-are-folds-in-cross-validation","title":"What are 'Folds' in Cross-Validation?","text":"<p>In the context of Cross-Validation, the term \"fold\" refers to each separate iteration, or \"split\", of the dataset.</p> <ul> <li>When you perform 5-fold cross-validation, you divide your data into 5 parts (folds).</li> <li>In each iteration (fold), a different part of the data is held out as the test set, and the remaining parts are combined to form the training set.</li> <li>The model is trained on this training set and evaluated on the test set.</li> <li>This process is repeated 5 times (for 5-fold CV), each time with a different part being the test set.</li> </ul>"},{"location":"models/cross-validation/#why-5-folds","title":"Why 5 Folds?","text":"<p>Why did we use 5 folds in our project?</p> <p>The number of folds is a parameter that you can choose. Five is a common choice because it's a good balance between computational efficiency and reliability of the results. More folds typically mean a better estimate of model performance but require more computation. Fewer folds are quicker but might not estimate performance as accurately.</p>"},{"location":"models/cross-validation/#what-does-cross-validation-achieve","title":"What Does Cross-Validation Achieve?","text":"<p>By the end of the cross-validation process, you have an array of performance scores (e.g., accuracy, R^2, MAE, etc.) - one score for each fold. You can then look at the average of these scores to get a more robust understanding of how well your model performs. This average is less sensitive to the particularities of a single train-test split and is generally a better indicator of model performance on unseen data.</p>"},{"location":"models/cross-validation/#summary","title":"Summary","text":"<p>In summary, Cross-Validation is a method to robustly assess the performance of your machine learning model. By using multiple train-test splits (folds), it gives you a more reliable measure of your model's ability to generalize to new data.</p> <p>We only performed 5-fold cross-validation in our project, but you can also use other numbers of folds, such as 10-fold cross-validation. </p> <p> If you'd like to see our code for cross-validation, go to the model's section of this documentation (located to the left) and click on the link to the model you're interested in. You'll find the code for cross-validation in the `Code' section of the model's documentation, it should take you to that model's notebook on GitHub, scroll down to the bottom of the notebook to find the code for cross-validation.</p>"},{"location":"models/naive/","title":"Naive Forecast","text":"<p>The Naive Forecast method is one of the simplest forecasting techniques which assumes that the most recent observation is the only important one and forecasts the same value for all future points.</p>"},{"location":"models/naive/#simple-explanation","title":"Simple Explanation","text":"<p>Imagine you are trying to predict the number of apples you'll sell tomorrow. A Naive Forecast would say: \"However many you sold today, that's how many you'll sell tomorrow.\" It's quick, straightforward, and sometimes surprisingly effective as a baseline.</p>"},{"location":"models/naive/#formula","title":"Formula","text":"<p>The formula for the Naive Forecast for a time series is simply:</p> <pre><code>\u0177\u209c\u208a\u2081 = y\u209c\n</code></pre> <p>where <code>\u0177\u209c\u208a\u2081</code> is the forecast for the next period, and <code>y\u209c</code> is the actual observation at the current time period.</p>"},{"location":"models/naive/#parameters","title":"Parameters","text":"<p>There are no parameters to tune in a Naive Forecast model, which is part of what makes it so simple.</p>"},{"location":"models/naive/#feature-importance","title":"Feature Importance","text":"<p>Feature importance does not apply to Naive Forecast because it does not use any features or variables other than the past value of the series itself.</p> <p>The Naive Forecast is often used as a benchmark for more complex forecasting models. If a sophisticated model can't beat the Naive Forecast, it might be too complex or not suitable for the data at hand.</p>"},{"location":"models/naive/#code","title":"Code","text":"<p>To see how we implemented Naive Forecast in our project NOT including cross-validation, check out the Naive Forecast notebook.</p>"},{"location":"models/overview/","title":"Models Overview","text":"<p>In our quest to develop an effective machine learning solution for our project, we selected a variety of models, each with unique characteristics and strengths. Our approach was to implement different types of models to understand their performance on our dataset and grasp the nuances of each.</p>"},{"location":"models/overview/#cycle-1-initial-modeling","title":"Cycle 1: Initial Modeling","text":"<ol> <li> <p>XGBoost    XGBoost, or Extreme Gradient Boosting, was our choice due to its popularity and proven track record in winning Kaggle competitions. It's known for its speed and performance, as well as its ability to handle missing data. We anticipated that XGBoost would provide us with a robust and accurate model, even with the less-than-perfect quality of our initial dataset.</p> </li> <li> <p>Random Forest (RF)    Random Forest was another model we opted for, primarily for its simplicity and effectiveness. It works well for both classification and regression tasks and has the added benefit of feature importance evaluation, which we considered essential for understanding our dataset better. Its ensemble nature, through bootstrapping and averaging predictions, helps in reducing overfitting, making it a reliable choice.</p> </li> <li> <p>Polynomial Regression    We chose Polynomial Regression to provide a contrast to the ensemble methods. It's a type of linear regression that models the relationship between the independent variable x and the dependent variable y as an nth degree polynomial. We were curious to see how a more straightforward, parametric model would perform on our dataset, particularly in capturing any non-linear patterns.</p> </li> <li> <p>Naive Forecast (Baseline)    To establish a baseline performance, we implemented a Naive Forecasting model. This simple model set the stage for us to better evaluate the performance improvements brought about by the more complex models. It served as a reality check, ensuring that our advanced models were indeed adding value and not just fitting to noise.</p> </li> </ol>"},{"location":"models/overview/#cycle-2-time-series-modeling","title":"Cycle 2: Time Series Modeling","text":"<p>As we progressed to the second cycle of our project, our focus shifted to time series models, acknowledging the temporal nature of our dataset.</p> <ol> <li> <p>ARIMA (AutoRegressive Integrated Moving Average)    ARIMA is a popular and widely used statistical method for time series forecasting. It captures the underlying patterns and structures in the time series data, making it a suitable choice for our project. We were interested in evaluating how well ARIMA could handle the trends and seasonality in our dataset.</p> </li> <li> <p>Prophet    Developed by Facebook, Prophet is specifically designed for forecasting time series data that exhibit strong seasonal patterns. We were drawn to its robustness and ability to handle missing data, as well as its ease of use. Prophet\u2019s ability to provide interpretable parameters was an added bonus, aiding us in understanding the model\u2019s predictions.</p> </li> <li> <p>XGBoost (Time Series)    Given the success of XGBoost in our initial cycle, we decided to adapt it for time series forecasting. We modified the data preparation and feature engineering steps to suit the temporal nature of our data, eager to see if XGBoost\u2019s power could be harnessed in a time series context.</p> </li> </ol> <p>By diversifying our selection of models, we aimed to gain a comprehensive understanding of different approaches to machine learning and time series forecasting. Each model brought its own set of challenges and learnings, contributing to our growing in the field. Through this approach, we not only aimed to achieve the best possible performance on our project but also to equip ourselves with the knowledge and skills that will be invaluable in our future endeavors in data science.</p>"},{"location":"models/polynomial-regression/","title":"Polynomial Regression Model","text":"<p>Polynomial Regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. This method provides a way to model a non-linear relationship between the dependent and independent variables.</p>"},{"location":"models/polynomial-regression/#simple-explanation","title":"Simple Explanation","text":"<p>Polynomial Regression is like giving a line in our graph the freedom to curve and bend. It\u2019s especially useful when the relationship between variables isn't straight-forward or linear.</p>"},{"location":"models/polynomial-regression/#formula","title":"Formula","text":"<p>The model's formula is:</p> <pre><code>y = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2081\u00b2 + ... + \u03b2\u2099x\u2081\u207f + \u03b5\n</code></pre> <p>Here, <code>y</code> represents the dependent variable we're trying to predict, <code>x\u2081</code> is the independent variable, <code>\u03b2\u2080</code> is the y-intercept, <code>\u03b2\u2081</code> to <code>\u03b2\u2099</code> are the coefficients for each degree of the independent variable, and <code>\u03b5</code> is the error term.</p>"},{"location":"models/polynomial-regression/#parameters","title":"Parameters","text":"<p>For our project we only used one parameter, <code>degree</code>, which is the degree of the polynomial. This determines the curvature of the fit.</p> <ul> <li>The degree of the polynomial (<code>n</code>), which determines the curvature of the fit.</li> </ul> <p>Another key parameter (which we did not use for simplicity) is:</p> <ul> <li>The coefficients (<code>\u03b2\u2081</code> to <code>\u03b2\u2099</code>), which are adjusted during the training process to fit the curve to the data.</li> </ul> <p>Unlike some other models, Polynomial Regression does not have an intrinsic feature importance metric. However, the size of the coefficients can give some indication of the importance of each term.</p>"},{"location":"models/polynomial-regression/#feature-importance","title":"Feature Importance","text":"<p>Polynomial Regression doesn't directly provide feature importance. However, the magnitude of the coefficients can imply how much influence each degree of the independent variable has on the dependent variable.</p> <p>For a detailed understanding of Polynomial Regression, you can refer to the documentation.</p>"},{"location":"models/polynomial-regression/#code","title":"Code","text":"<p>To see how we implemented Polynomial Regression in our project including cross-validation, check out the Polynomial Regression notebook.</p>"},{"location":"models/prophet/","title":"Prophet Model","text":"<p>Prophet is a forecasting tool designed by Facebook for time series data that is especially good at handling data with strong seasonal effects and several seasons of historical data.</p>"},{"location":"models/prophet/#simple-explanation","title":"Simple Explanation","text":"<p>Imagine if you were trying to predict the number of guests who will visit a restaurant every day. You notice patterns like more guests on weekends or on holidays. Prophet is like a smart assistant that helps you account for these regular patterns (seasonality), trends over time (like growth), and special events (like a festival) to predict future attendance.</p>"},{"location":"models/prophet/#formula","title":"Formula","text":"<p>Prophet models time series data with the following components:</p> <pre><code>y(t) = g(t) + s(t) + h(t) + e(t)\n</code></pre> <p>where:</p> <ul> <li><code>y(t)</code> is the predicted value.</li> <li><code>g(t)</code> represents the trend component, modeling non-periodic changes.</li> <li><code>s(t)</code> represents periodic changes (weekly, yearly, etc.).</li> <li><code>h(t)</code> represents the effects of holidays which can be specified by the user.</li> <li><code>e(t)</code> is the error term.</li> </ul>"},{"location":"models/prophet/#parameters","title":"Parameters","text":"<p>Prophet focuses mainly on seasonal effects and trend changes, so it doesn't have many parameters. And the parameters it does have are mostly for tuning the model's seasonality and trend components. </p> <p>For simplicity, we did not use any of these parameters in our project. However, here is quick overview of the parameters:</p> <p>Prophet does not have traditional model parameters like <code>p</code>, <code>d</code>, <code>q</code> in ARIMA. Instead, it has several adjustable components:</p> <ul> <li><code>growth</code>: Can be 'linear' or 'logistic' to specify a capacity to which the forecast can grow.</li> <li><code>seasonality</code>: Prophet will by default fit weekly and yearly seasonality, if the time series is more than two cycles long.</li> <li><code>holidays</code>: You can add custom holidays and events.</li> </ul>"},{"location":"models/prophet/#feature-importance","title":"Feature Importance","text":"<p>Prophet automatically detects and accounts for seasonality, thus there's no traditional feature importance. The model's interpretability comes from the decomposition of the forecast into trend, seasonality, and holiday components.</p>"},{"location":"models/prophet/#code","title":"Code","text":"<p>To see how we implemented Prophet in our project including cross-validation, check out the Prophet notebook.</p>"},{"location":"models/prophet/#additional-notes","title":"Additional Notes","text":"<ul> <li>Prophet is robust to missing data and shifts in the trend and typically handles outliers well.</li> <li>It is also easy to use with intuitive parameters and practices.</li> </ul> <p>For an in-depth understanding and tutorials on Prophet, you can visit the Prophet documentation.</p>"},{"location":"models/random-forest/","title":"Random Forest Model","text":"<p>Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>"},{"location":"models/random-forest/#simple-explanation","title":"Simple Explanation","text":"<p>Random Forest is like a team of decision-makers (trees) where each member (tree) gives a vote (prediction), and the final decision is made based on the majority vote. It's a powerful and versatile machine learning method that can be used for both classification and regression tasks.</p> <p>The \"forest\" is made up of many \"trees\" (hence the name), but unlike real trees, these decision trees are simple models that make predictions based on the features of the data. The \"random\" part comes from the fact that each tree in the forest is built from a random sample of the data, and at each node in the tree, a random subset of features is considered for splitting. This randomness helps make the model more robust to noise in the data and less likely to overfit, meaning it can generalize better to new, unseen data.</p> <p>In simpler terms, you can think of Random Forest as a way of asking many different experts for their opinion and then taking the average of all those opinions to make a final decision. Each expert has a different piece of the overall picture, and by combining their knowledge, you get a well-rounded answer.</p>"},{"location":"models/random-forest/#formula","title":"Formula","text":"<p>The prediction of a Random Forest model for a regression problem can be expressed as the average of the outputs of all the individual trees in the forest.  For a classification problem, it's the majority vote (mode) among the predictions from all trees.</p> <p>For a regression Random Forest, the predicted value y-hat for a given instance x is calculated as:</p> <pre><code>y-hat = (1/N) * \u03a3(tree_k.predict(x)) for k = 1 to N\n</code></pre> <p>where: </p> <ul> <li><code>N</code> is the number of trees in the forest.</li> <li><code>tree_k.predict(x)</code> is the prediction for instance <code>x</code> from the <code>k-th</code> tree.</li> </ul> <p>For a detailed explanation of the Random Forest algorithm and its parameters, you can visit the Random Forest section in scikit-learn's documentation.</p>"},{"location":"models/random-forest/#parameters","title":"Parameters","text":"<ul> <li><code>n_estimators</code>: The number of trees in the forest.</li> <li><code>max_depth</code>: The maximum depth of the trees.</li> <li><code>min_samples_leaf</code>: The minimum number of samples required to be at a leaf node.</li> <li><code>max_features</code>: The number of features to consider when looking for the best split.</li> </ul> <p>For a detailed explanation of the Random Forest algorithm and its parameters, you can visit the Random Forest section in scikit-learn's documentation.</p>"},{"location":"models/random-forest/#feature-importance","title":"Feature Importance","text":"<p>One of the most powerful aspects of Random Forest is its intrinsic ability to rank the importance of features based on how much they improve the purity of the node. This means that more important features will be selected more often when splitting nodes. You can access feature importance in scikit-learn with the <code>feature_importances_</code> attribute after fitting a Random Forest model.</p> <p>Random Forest can capture the importance of features with respect to the task being solved, which is a handy attribute for feature selection and understanding the model.</p>"},{"location":"models/random-forest/#code","title":"Code","text":"<p>To see how we implemented Random Forest in our project including cross-validation, check out the Random Forest notebook.</p>"},{"location":"models/xgboost/","title":"XGBoost Model","text":"<p>XGBoost stands for eXtreme Gradient Boosting. It's an advanced implementation of gradient boosting that is efficient, flexible, and portable. XGBoost is particularly popular due to its speed and performance and is widely used in machine learning competitions for structured or tabular data.</p> <p>XGBoost improves upon the base gradient boosting by handling null values and regularizing to avoid overfitting. It's known for its scalability that handles various types of predictive modeling problems efficiently.</p>"},{"location":"models/xgboost/#simple-explanation","title":"Simple Explanation","text":"<p>At its core, XGBoost builds multiple decision trees sequentially, where each tree tries to correct the mistakes of the previous ones. The 'boosting' part of the name refers to this idea of building up a strong predictive model by combining many weaker models.</p>"},{"location":"models/xgboost/#formula","title":"Formula","text":"<p>The essence of XGBoost lies in constructing an ensemble of decision trees, with each tree being a series of decisions that lead to a final prediction. When these trees are combined, the final prediction is derived by aggregating the predictions of each individual tree.</p> <p>The simplified formula for the predicted output <code>(y-hat)</code> for each instance <code>i</code> in XGBoost is given by the sum of the predictions from <code>N</code> trees:</p> <pre><code>y-hat_i = \u03a3(f_k(x_i)) for k = 1 to N\n</code></pre> <p>where:</p> <ul> <li><code>y-hat_i</code> is the predicted value for the <code>i-th</code> instance. </li> <li><code>f_k</code> represents the k-th decision tree's prediction function.</li> <li><code>x_i</code> is the feature vector for the <code>i-th</code> instance.</li> <li><code>N</code> is the number of trees in the ensemble.</li> </ul> <p>Each decision tree (<code>f_k</code>) is constructed to minimize the overall loss function, which includes a regularization term to control the model's complexity.</p> <p>Due to the intricate nature of the complete formula, incorporating elements like gradients and regularization, it is quite extensive. For a more detailed mathematical exposition of XGBoost's predictive model, refer to the XGBoost documentation on the algorithm.</p>"},{"location":"models/xgboost/#parameters","title":"Parameters","text":"<p>While XGBoost has many parameters, we'll focus on the ones most impactful for our project:</p> <ul> <li> <p><code>n_estimators</code>: This represents the number of trees you want to build before taking the maximum voting or averages of predictions. Higher numbers of trees can improve the model but can also make your model slower.</p> </li> <li> <p><code>learning_rate</code>: Also known as the 'eta' value, it's used to prevent overfitting by making the model more robust by shrinking the weights on each step.</p> </li> <li> <p><code>max_depth</code>: This determines how deeply each tree is allowed to grow during any boosting round. Deeper trees can model more complex patterns but can also lead to overfitting.</p> </li> </ul> <p>There are more parameters, but these are the key ones we used. For a deeper understanding, refer to the XGBoost documentation.</p>"},{"location":"models/xgboost/#feature-importance","title":"Feature Importance","text":"<p>XGBoost provides a way to examine the importance of each feature in the final decision. This is helpful for understanding the model and for feature selection.</p> <p><code>Gain</code> is the improvement in accuracy brought by a feature to the branches it is on. The higher the gain, the more important the feature is.  Refer to the XGBoost documentation for more details on feature importance.</p>"},{"location":"models/xgboost/#xgboost-for-time-series","title":"XGBoost for Time Series","text":"<p>XGBoost can be used for time series prediction (as we did in Cycle 2) by transforming the time series forecasting problem into a supervised learning problem. This involves creating features from the time series data like lag features, rolling window statistics, and others to capture time-dependent patterns. For this, feature engineering is crucial, where past observations are used as input features to predict future values.</p> <p>To apply XGBoost to time series, it's important to ensure that the model is trained on past data to predict future data, respecting the time series nature of the problem.</p> <p>Key considerations when using XGBoost for time series:</p> <ul> <li>Lag features: Include previous time steps as features to provide the model with information about the trend and seasonality.</li> <li>Rolling window statistics: Use statistics like mean or variance on a window of past observations as features to capture the local temporal dynamics.</li> <li>Date-time features: Extract information from the timestamp, like the day of the week or the hour of the day, which can affect the time series patterns.</li> </ul> <p>To learn more about using XGBoost for time series forecasting, check out this comprehensive guide.</p>"},{"location":"models/xgboost/#code","title":"Code","text":"<p>To see how we implemented XGBoost in our project including cross-validation, check out the XGBoost(Regression) notebook or the XGBoost(Time Series) notebook.</p>"},{"location":"web-interface/endpoints/","title":"Web Interface: Endpoints Documentation","text":"<p>The web interface for our oil price prediction and analysis tool is powered by a Flask application, providing a series of endpoints to interact with the underlying data and models. Below is a documentation of the current endpoints and their functionalities.</p>"},{"location":"web-interface/endpoints/#home-page-endpoint","title":"Home Page Endpoint (<code>/</code>)","text":"<ul> <li>Method: GET</li> <li>Description: This endpoint renders the home page of the web interface.</li> <li>Response: HTML content of the <code>index.html</code> page.</li> </ul>"},{"location":"web-interface/endpoints/#feature-names-endpoint-features","title":"Feature Names Endpoint (<code>/features</code>)","text":"<ul> <li>Method: GET</li> <li>Description: This endpoint provides the names of the features available in the dataset, which are used to populate the dropdown menu in the user interface.</li> <li>Response: A JSON array of feature names.</li> </ul>"},{"location":"web-interface/endpoints/#data-endpoint-data","title":"Data Endpoint (<code>/data</code>)","text":"<ul> <li>Method: POST</li> <li>Description: This endpoint filters the dataset based on the selected features received in the request and returns the filtered data.</li> <li>Request Body: A JSON object with a key <code>selected_features</code> containing an array of selected feature names.</li> <li>Response: A JSON object representing the filtered dataset.</li> </ul>"},{"location":"web-interface/endpoints/#plot-endpoint-plot","title":"Plot Endpoint (<code>/plot</code>)","text":"<ul> <li>Method: POST</li> <li>Description: This endpoint generates a plot based on the selected features and plot type. It returns the plot and its legend as base64-encoded PNG images.</li> <li>Request Body: A JSON object with keys <code>selected_features</code> (array of feature names) and <code>plot_type</code> (string specifying the type of plot, e.g., 'line' or 'scatter').</li> <li>Response: A JSON object with keys <code>plot_url</code> and <code>legend_url</code>, containing base64-encoded PNG images of the plot and legend, respectively.</li> </ul>"},{"location":"web-interface/endpoints/#data-explorer-page-endpoint-data_explorer","title":"Data Explorer Page Endpoint (<code>/data_explorer</code>)","text":"<ul> <li>Method: GET</li> <li>Description: This endpoint renders the Data Explorer page of the web interface.</li> <li>Response: HTML content of the <code>data_explorer.html</code> page.</li> </ul>"},{"location":"web-interface/endpoints/#prediction-page-endpoint-predict","title":"Prediction Page Endpoint (<code>/predict</code>)","text":"<ul> <li>Method: GET</li> <li>Description: This endpoint renders the Prediction page of the web interface.</li> <li>Response: HTML content of the <code>predict.html</code> page.</li> </ul>"},{"location":"web-interface/endpoints/#usage-example","title":"Usage Example:","text":"<ul> <li>To retrieve feature names:<ul> <li>Access the <code>/features</code> endpoint.</li> </ul> </li> </ul>"},{"location":"web-interface/endpoints/#conclusion","title":"Conclusion","text":"<p>These endpoints collectively form the backbone of the web interface, enabling users to interactively explore data, generate plots, and perform predictions. The Flask application ensures that these functionalities are seamlessly integrated, providing a user-friendly experience.</p>"},{"location":"web-interface/features/","title":"Current Features of the Website","text":"<p>The web interface of our oil price prediction and analysis tool is designed to be user-friendly and interactive, allowing users to explore data, visualize relationships, and make predictions. Below is a detailed description of the current features available on the website.</p>"},{"location":"web-interface/features/#interactive-data-exploration","title":"Interactive Data Exploration","text":"<ul> <li>Description: Users can select different features from a dropdown menu to visualize their relationship with real oil prices.</li> <li>Functionality: The website provides a dynamic plot that updates based on the user\u2019s selection of features. This helps in understanding how different macroeconomic indicators and other features correlate with oil prices.</li> </ul>"},{"location":"web-interface/features/#data-filtering","title":"Data Filtering","text":"<ul> <li>Description: Users can filter the dataset based on selected features.</li> <li>Functionality: Upon selecting the desired features, the website sends a request to the server to filter the dataset accordingly and returns the filtered data for further analysis or visualization.</li> </ul>"},{"location":"web-interface/features/#visualizations","title":"Visualizations","text":"<ul> <li>Description: The website provides different types of plots (line, scatter) to visualize the relationship between selected features and real oil prices.</li> <li>Functionality: Users can choose the type of plot they prefer, and the website will generate and display the plot accordingly. The plots include markers for the start and end points of each feature, providing additional context.</li> </ul>"},{"location":"web-interface/features/#downloadable-plots","title":"Downloadable Plots","text":"<ul> <li>Description: Users have the option to save the plots as PNG images for future reference if needed.</li> <li>Functionality: The website encodes the plots as base64 PNG images, which can be easily downloaded.</li> </ul>"},{"location":"web-interface/features/#prediction-interface","title":"Prediction Interface","text":"<ul> <li>Description: Users can make predictions based on selected models and input features.</li> <li>Functionality: The prediction page allows users to choose a model, ~~input the necessary features~~, and receive a prediction of future oil prices. The website interfaces with the backend to perform the prediction and returns the result to the user.</li> </ul>"},{"location":"web-interface/features/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>Description: Users can evaluate the performance of different models based on historical data.</li> <li>Functionality: The website provides a way to compare different models, showing their performance metrics and helping users decide which model might be the most suitable for their needs.</li> </ul>"},{"location":"web-interface/features/#user-friendly-interface","title":"User-Friendly Interface","text":"<ul> <li>Description: The website is designed to be intuitive and easy to navigate, ensuring a smooth user experience.</li> <li>Functionality: Clear labels, responsive design, and helpful tooltips guide users through the various features and functionalities of the website.</li> </ul>"},{"location":"web-interface/features/#responsive-design","title":"Responsive Design","text":"<ul> <li>Description: The website is designed to be accessible and functional across various devices and screen sizes.</li> <li>Functionality: Regardless of whether a user is accessing the website from a desktop computer, tablet, or smartphone, the interface adjusts to provide an optimal viewing experience.</li> </ul>"},{"location":"web-interface/features/#dont-see-a-feature-you-want","title":"Don't see a feature you want?","text":"<p>If you have any suggestions for new features or improvements, please let us know! We are always looking for ways to improve our website and make it more useful for our users. Just create a new issue on our GitHub repository, and we will get back to you as soon as possible.</p>"},{"location":"web-interface/process/","title":"Web Interface Process","text":""},{"location":"web-interface/process/#our-process-creating-the-web-interface","title":"Our Process: Creating the Web Interface","text":"<p>The development of the web interface for our oil price prediction and analysis tool was a thorough process that involved several stages. We aimed to create an interface that was both user-friendly and easy to navigate, providing users with powerful tools for data exploration, visualization, and prediction. Below is an overview of our development process.</p>"},{"location":"web-interface/process/#1-planning-and-design","title":"1. Planning and Design","text":"<ul> <li>Objective: Define the purpose of the web interface, outline the key features, and design the user experience.</li> <li>Actions:<ul> <li>Conducted brainstorming sessions to list down all the potential features and functionalities.</li> <li>Created wireframes to visualize the user interface and user experience.</li> <li>Defined the workflow and interactions for different features.</li> </ul> </li> <li>Tools &amp; Resources: Paper &amp; pencil for wireframing, GitHub for project tracking.</li> </ul>"},{"location":"web-interface/process/#2-setting-up-the-development-environment","title":"2. Setting Up the Development Environment","text":"<ul> <li>Objective: Prepare all necessary tools and libraries for development.</li> <li>Actions:<ul> <li>Initialized a new Flask project as the framework for our web application.</li> <li>Set up a virtual environment and installed necessary Python libraries like Pandas, NumPy, and Matplotlib.</li> <li>Configured the development server for local testing.</li> </ul> </li> <li>Tools &amp; Resources: Flask, Conda for environment management, PyCharm as the code editor.</li> </ul>"},{"location":"web-interface/process/#3-data-integration","title":"3. Data Integration","text":"<ul> <li>Objective: Integrate the dataset and ensure it can be accessed and manipulated through the web interface.</li> <li>Actions:<ul> <li>Loaded the oil price dataset into a Pandas DataFrame.</li> <li>Created endpoints in Flask to expose the data to the front end.</li> <li>Ensured data integrity and handled any missing or infinite values.</li> </ul> </li> <li>Tools &amp; Resources: Pandas for data manipulation, Flask for creating API endpoints.</li> </ul>"},{"location":"web-interface/process/#4-developing-core-functionalities","title":"4. Developing Core Functionalities","text":"<ul> <li>Objective: Implement the main features of the web interface.</li> <li>Actions:<ul> <li>Developed endpoints for data filtering, feature retrieval, and plotting.</li> <li>Implemented the front-end logic to interact with these endpoints.</li> <li>Created dynamic visualizations based on user input.</li> </ul> </li> <li>Tools &amp; Resources: JavaScript for front-end development, Matplotlib for plotting, Flask for API endpoints.</li> </ul>"},{"location":"web-interface/process/#5-user-interface-development","title":"5. User Interface Development","text":"<ul> <li>Objective: Develop and refine the user interface based on the design wireframes.</li> <li>Actions:<ul> <li>Implemented the layout, navigation, and user input elements.</li> <li>Ensured a responsive design for accessibility on various devices.</li> <li>Conducted user testing to identify and fix usability issues.</li> </ul> </li> <li>Tools &amp; Resources: HTML, CSS, TailwindCSS for styling, JavaScript for interactivity.</li> </ul>"},{"location":"web-interface/process/#6-testing-and-debugging","title":"6. Testing and Debugging","text":"<ul> <li>Objective: Ensure that all features work correctly and efficiently.</li> <li>Actions:<ul> <li>Conducted thorough testing of all features and user interactions.</li> <li>Debugged any issues found during testing.</li> <li>Integrated unit testing to ensure code quality and reliability.</li> <li>Implemented continuous integration to automate testing upon code changes.</li> </ul> </li> <li>Tools &amp; Resources: Browser Developer Tools, Flask debugging tools, PyTest for unit testing and GitHub Actions for continuous integration.</li> </ul>"},{"location":"web-interface/process/#7-documentation-and-deployment","title":"7. Documentation and Deployment","text":"<ul> <li>Objective: Prepare documentation for users and deploy the web interface.</li> <li>Actions:<ul> <li>Wrote comprehensive documentation on how to use the web interface.</li> <li>Deployed the web application to a web server.</li> <li>Ensured all dependencies were properly configured for deployment.</li> </ul> </li> <li>Tools &amp; Resources: GitHub Pages for deployment of documentation, Render for deployment of web application. MkDocs for documentation.</li> </ul>"},{"location":"web-interface/process/#8-continuous-improvement-and-updates","title":"8. Continuous Improvement and Updates","text":"<ul> <li>Objective: Keep the web interface up-to-date and continuously improve based on user feedback.</li> <li>Actions:<ul> <li>Implemented updates and new features as required.</li> <li>Conducted regular performance and security audits.</li> </ul> </li> <li>Tools &amp; Resources: GitHub for version control.</li> </ul>"},{"location":"web-interface/process/#conclusion","title":"Conclusion","text":"<p>The development of the web interface is an ongoing collaborative effort that requires careful planning, development, and testing. By following this structured process, we were able to create an interface that is not only functional but also intuitive and user-friendly. The source code and additional documentation can be found on our GitHub repository.</p>"},{"location":"web-interface/purpose/","title":"Web Interface: Purpose and Evolution","text":""},{"location":"web-interface/purpose/#introduction","title":"Introduction","text":"<p>The initial purpose of our web interface was aimed for a straightforward yet vital objective: to visually explore and understand the correlation between various features and the real oil price. Which later was planned to evolve into a more comprehensive platform that supports model predictions and evaluations.</p>"},{"location":"web-interface/purpose/#initial-purpose-correlation-exploration","title":"Initial Purpose: Correlation Exploration","text":"<p>The initial version of our web interface was designed to provide a user-friendly environment where users could select one or more features and observe their correlation with real oil prices. This functionality was crucial for several reasons:</p> <ul> <li> <p>Simplifying Complexity: The relationships between different features and oil prices can be intricate and varied. The web interface sought to simplify these relationships, making them accessible and understandable with  visualizations and interactive controls.</p> </li> <li> <p>Interactive Learning: By interactively selecting features and observing their correlation with oil prices, users could develop a more nuanced understanding of the factors influencing oil prices. This active participation in data exploration fosters a deeper and more engaged learning experience.</p> </li> <li> <p>Data-Driven Decisions: Armed with a better understanding of the data, users can make more informed decisions and predictions. The web interface provides a platform for users to explore the data and gain insights that can be applied to data-driven decision-making.</p> </li> </ul>"},{"location":"web-interface/purpose/#evolution-from-exploration-to-prediction-and-evaluation","title":"Evolution: From Exploration to Prediction and Evaluation","text":"<p>As our project matured and our understanding of machine learning deepened, so too did the capabilities of our web interface. It evolved from a tool for simple data correlation exploration to a comprehensive platform that supports model predictions and evaluations. This transformation was driven by a desire to provide more value and a more holistic experience for our users.</p> <ul> <li> <p>Model Integration: Users can now leverage the power of machine learning models directly from the web interface. This integration allows for real-time predictions, giving users instant insights and forecasts based on the data and models.</p> </li> <li> <p>Evaluation Metrics: Alongside predictions, the web interface provides various evaluation metrics, helping users to assess the accuracy of the models. This feature is crucial for building trust in the predictions and ensuring that users have all the information they need to make informed decisions.</p> </li> <li> <p>Enhanced User Experience: The evolution of the web interface has been accompanied by improvements in usability and design. The goal was to create an intuitive and user-friendly platform, reducing barriers to entry and ensuring that users of all backgrounds can easily navigate and utilize the tool.</p> </li> </ul>"},{"location":"web-interface/purpose/#conclusion","title":"Conclusion","text":"<p>The web interface stands as a testament to our project's growth and our commitment to providing practical and impactful solutions. What began as a tool for correlation exploration has blossomed into a multifaceted platform, empowering users to not only understand the past and present but also to peer into the future with advanced model predictions.</p> <p>By continually adapting and expanding the capabilities of our web interface, we strive to provide a valuable resource for anyone looking to explore the intricate world of machine learning, from novices and students to experienced analysts and decision-makers.</p>"},{"location":"web-interface/user-guide/","title":"User Guide: How to Use the Oil Price Prediction Web Interface","text":"<p>Hello there! Welcome to our Oil Price Prediction Wb Interface user guide. We've made this tool to help you understand how different things in the world can affect oil prices. And guess what? You don't need to be a big scientist to use it! In this guide, we'll show you step by step how to become a pro at using our interface. Let's dive in!</p>"},{"location":"web-interface/user-guide/#step-1-open-the-website","title":"Step 1: Open the Website","text":"<p>First, you need to open your web browser and go to our website: oil-price-prediction.onrender.com. Just type the web address or click on the link, and voil\u00e0, you're there!</p>"},{"location":"web-interface/user-guide/#step-2-choose-what-you-want-to-look-at","title":"Step 2: Choose What You Want to Look At","text":"<p>Our interface has a lot of cool features that show you how different things (\"features\") like money, weather, or big world events can affect oil prices. Here's how you can choose what you want to look at:</p>"},{"location":"web-interface/user-guide/#if-you-want-to-explore-data","title":"If You Want to Explore Data:","text":"<ol> <li>Click on the \u201cData Explorer\u201d button. This will take you to a new page.</li> <li>You\u2019ll see a list of features. Click on the ones you\u2019re interested in to select them.</li> <li>Wait for the magic to happen. A plot will appear that shows you how the features you picked change over time. You can also download the plot if you want to save it for later.</li> </ol>"},{"location":"web-interface/user-guide/#if-you-want-to-make-predictions","title":"If You Want to Make Predictions:","text":"<ol> <li>Click on the \u201cPredict\u201d button. This will also take you to a new page.</li> <li>Pick the model you want to use to make predictions.</li> <li>Click on \u201cMake Prediction\u201d and wait for the magic to happen. You\u2019ll see a plot that shows you the model\u2019s predictions for the future. You can also download the plot if you want to save it for later. Along with the plot, you\u2019ll also see some numbers that tell you how good the model is at making predictions.</li> </ol>"},{"location":"web-interface/user-guide/#step-3-look-at-cool-charts","title":"Step 3: Look at Cool Charts","text":"<p>We love charts because they help us see patterns and understand things better. Once you\u2019ve picked your features, you can look at different charts that show you how these features and oil prices change over time. You can pick if you want to see a line chart or a scatter plot.</p>"},{"location":"web-interface/user-guide/#step-4-ask-questions-or-get-help","title":"Step 4: Ask Questions or Get Help","text":"<p>If something is confusing or not working, don\u2019t worry! You can always ask for assistance. Just click on the \u201cHelp\u201d button, and you\u2019ll see a list of frequently asked questions or just navigate to our FAQ section.</p>"},{"location":"web-interface/user-guide/#step-5-have-fun-and-learn","title":"Step 5: Have Fun and Learn!","text":"<p>The most important thing is to have fun and be curious. Try different features, look at the charts, and see if you can find patterns or make predictions on your own.</p> <p>And that's it! You're now ready to use our Oil Price Prediction Web Interface like a pro. Go ahead, start exploring and have a great time learning!</p>"}]}